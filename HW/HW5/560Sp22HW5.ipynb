{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpa/2eDNH0IXfmJYUNxZzZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yexf308/AppliedStochasticProcess/blob/main/HW/HW5/560Sp22HW5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVkmK8fXL-My"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\def\\m#1{\\mathbf{#1}}$\n",
        "$\\def\\mm#1{\\boldsymbol{#1}}$\n",
        "$\\def\\mb#1{\\mathbb{#1}}$\n",
        "$\\def\\c#1{\\mathcal{#1}}$\n",
        "$\\def\\mr#1{\\mathrm{#1}}$\n",
        "$\\newenvironment{rmat}{\\left[\\begin{array}{rrrrrrrrrrrrr}}{\\end{array}\\right]}$\n",
        "$\\newcommand\\brm{\\begin{rmat}}$\n",
        "$\\newcommand\\erm{\\end{rmat}}$\n",
        "$\\newenvironment{cmat}{\\left[\\begin{array}{ccccccccc}}{\\end{array}\\right]}$\n",
        "$\\newcommand\\bcm{\\begin{cmat}}$\n",
        "$\\newcommand\\ecm{\\end{cmat}}$\n",
        "# Homework 5\n",
        "## Homework guideline\n",
        "- The deadline is May 5th 10:30am. Submission after the deadline will not be graded. \n",
        "\n",
        "\n",
        "- Submit your work(your reasoning and your code) as a SINGLE .ipynb document. Please rename the document as _HW1_YOURNAME.ipynb_ (for example, _HW1_FELIX.ipynb_). You are responsible for checking that you have correctly submitted the correct document. If your code cannot run, you may receive NO point. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- You only use the Python packages included in the following cell. You are not allowed to use other advanced package or modules unless you are permitted to. \n",
        "\n",
        "- In your final submission include the plots produced by the unedited code as presented below, as well as any additional plots produced after editing the code during the course of a problem. You may find it necessary to copy/paste relevant code into additional cells to accomplish this.\n",
        "\n",
        "- Feel free to use the lecture notes and other resources. But you\n",
        "must understand, write, and hand in your own answers. In addition, you must write and submit\n",
        "your own code in the programming part of the assignment (we may run your code).\n",
        "If you copy someone else homework solution, both of you may receive ZERO point. \n",
        "\n",
        "\n",
        "- Colab is preferred. However, if you use Anaconda, please download the .mat file locally and save it to the same folder as this homework file. \n",
        "\n"
      ],
      "metadata": {
        "id": "x2IGXsiOMEwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1: Implement forward filtering backward sampling (50pt)\n",
        "We now consider the sampling of latent states $h$ given an observation $v$. Firstly, it is important to differentiate sampling from Viterbi-backtracking:\n",
        "* Viterbi-backtracking: $h_{1:n}^* = \\text{argmax } p(h_{1:n} | v_{1:n})$\n",
        "    * There is only one $h_{1:n}^*$ that has the largest probability (under the assumption of a unique global maximum).\n",
        "* Sampling: $h_{1:n} \\sim p(h_{1:n} | v_{1:n})$\n",
        "    * In sampling we attempt to generate several latent sequences $h_{1:n}$, not just the most probable.\n",
        "    * Note that the sequence with $h_{1:n}^*$ still has the largest probability to get sampled.\n",
        "\n",
        "\n",
        "It is also helpful to view sampling as a random function (as opposed to a deterministic function): \n",
        "* Viterbi-backtracking is a deterministic function since it always gives the same output.\n",
        "* Sampling is a random function since each time it may give us different outputs.\n",
        "\n",
        "Sampling from the posterior $p(h_{1:n} | v_{1:n})$ can be done with backward-sampling:\n",
        "\\begin{align}\n",
        "    h_n &\\sim p(h_n | v_{1:n}) \\\\ \n",
        "    h_{t-1} &\\sim p(h_{t-1} | h_t, v_{1:n}) \\qquad \\text{for }t\\le n\n",
        "\\end{align}\n",
        "\n",
        "In the previous Homework, we showed that \n",
        "\\begin{align}\n",
        "p(h_{t-1}| h_t, v_{1:n}) =\\frac{\\alpha(h_{t-1})}{\\alpha(h_t)}p(h_t| h_{t-1})p(v_t|h_t)\n",
        "\\end{align}\n",
        "Since the probabilities above are expressed as a function of the transition, emission, and the alpha variables, we will use the forward algorithm to obtain them.\n",
        "\n",
        "## Algorithm\n",
        "We thus obtain the following algorithm to generate samples from $p(h_1, \\dots, h_n | v_{1:n})$:\n",
        "\n",
        "- Run the filtering to determine all $\\alpha(h_t)$ forward in time for $t=1, \\dots, n$.\n",
        "\n",
        "- Sample $h_n$ from $p(h_n|v_{1:n})\\propto\\alpha(h_n)$.\n",
        "\n",
        "-  Go backwards in time using\n",
        "\\begin{align}\n",
        "p(h_{t-1}| h_t, v_{1:n}) =\\frac{\\alpha(h_{t-1})}{\\alpha(h_t)}p(h_t| h_{t-1})p(v_t|h_t)\n",
        "\\end{align}\n",
        "to generate samples $h_{t-1}| h_t, v_{1:n}$ for $t=n,\\dots, 2$.\n",
        "\n",
        "You will implement this algorithm in the following function. \n",
        "The computation of the equations above are performed in the log domain, hence\n",
        "\n",
        "\\begin{align*}\n",
        "\\log p(h_{t - 1}| h_t, v_{1:T}) &=  \\log \\alpha(h_{t-1}) + \\log p(h_t | h_{t-1}) + \\log p(v_t | h_t) - \\log \\alpha(h_t)\n",
        "\\end{align*}\n"
      ],
      "metadata": {
        "id": "3w4Cx0bGNKNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sampling(log_initial, log_transition, log_emission, v):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        initial: a vector of length N\n",
        "        transition: a matrix of shape N * N\n",
        "        emission: a matrix of shape V * N\n",
        "        v: observed sequence, a vector of length T\n",
        "        \n",
        "    Returns:\n",
        "        h: sampled sequence, a vector of length T\n",
        "    \"\"\"\n",
        "    return"
      ],
      "metadata": {
        "id": "kqoZPhGFpwSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the algorthm with examples\n",
        "### Test case 1\n",
        "we call the sampling function to see its output. Try running it multiple times to see that the outputs can be different."
      ],
      "metadata": {
        "id": "59Nk6jJ5qfiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_initial = np.log(np.array([0.3, 0.7]))\n",
        "log_transition = np.log(np.array([[0.2, 0.8], \n",
        "                                  [0.6, 0.4]]))\n",
        "log_emission = np.log(np.array([[0.6, 0.7], \n",
        "                                [0.4, 0.3]]))\n",
        "v = [0, 0]\n",
        "p_h, h = sampling(log_initial, log_transition, log_emission, v)\n",
        "print(np.prod(p_h), h)\n"
      ],
      "metadata": {
        "id": "ABTF1ufMrJyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test case 2\n",
        "We verify our implementation by the law of large numbers: if we call the sampling function many times, then the frequency of each sampled latent sequence will be approximately equal to their probability. \n",
        "\n",
        "Note that the frequency of the latent sequence [1, 0] returned from the simulation should close to 0.4045, i.e., approximately same as the true probability returned by the Viterbi algorithm. You may increase the number `N=100000`, which should return an even closer result."
      ],
      "metadata": {
        "id": "INadetFtrZ0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h_freq = {'00': 0, '01': 0, '10': 0, '11': 0}\n",
        "N = 10000\n",
        "for _ in range(N):\n",
        "    p_h, h = sampling(log_initial, log_transition, log_emission, v)\n",
        "    h_freq[f'{h[0]}{h[1]}'] += 1\n",
        "\n",
        "for h in h_freq:\n",
        "    print('latent %s freq %.4f' % (h, h_freq[h] / N))"
      ],
      "metadata": {
        "id": "8E2naEjXrhCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2: (optional) Correction to your previous homework question\n",
        "You may pick any one question in homework 1-4 that didn't perform well, and now you have the chance to correct your mistakes. If you successfully correct your mistakes, your previous grade will be replaced by the current score.\n",
        "\n",
        "State Your question that you want to correct:"
      ],
      "metadata": {
        "id": "T_2lmhQiMXgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your new code starts here"
      ],
      "metadata": {
        "id": "e0stlCDJM6bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your New Solution:"
      ],
      "metadata": {
        "id": "zXgyxmePM8VW"
      }
    }
  ]
}