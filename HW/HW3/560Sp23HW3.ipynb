{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPK63rQlybv6YbwblKmyFqb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yexf308/AppliedStochasticProcess/blob/main/HW/HW3/560Sp23HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGmU_39Swl-w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b606dd8b-3c25-471f-a813-297595c178e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ],
      "source": [
        "%pylab inline \n",
        "import numpy.linalg as LA\n",
        "import codecs, json\n",
        "from time import time\n",
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import requests\n",
        "import collections\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "Pm0HxUHc0eyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\def\\m#1{\\mathbf{#1}}$\n",
        "$\\def\\mm#1{\\boldsymbol{#1}}$\n",
        "$\\def\\mb#1{\\mathbb{#1}}$\n",
        "$\\def\\c#1{\\mathcal{#1}}$\n",
        "$\\def\\mr#1{\\mathrm{#1}}$\n",
        "$\\newenvironment{rmat}{\\left[\\begin{array}{rrrrrrrrrrrrr}}{\\end{array}\\right]}$\n",
        "$\\newcommand\\brm{\\begin{rmat}}$\n",
        "$\\newcommand\\erm{\\end{rmat}}$\n",
        "$\\newenvironment{cmat}{\\left[\\begin{array}{ccccccccc}}{\\end{array}\\right]}$\n",
        "$\\newcommand\\bcm{\\begin{cmat}}$\n",
        "$\\newcommand\\ecm{\\end{cmat}}$\n",
        "\n",
        "# Homework 3\n",
        "## Homework guideline\n",
        "\n",
        "- The deadline is March 31st 10:30am. Submission after the deadline will not be graded. \n",
        "\n",
        "- Submit your work(your reasoning and your code) as a SINGLE .ipynb document. Please rename the document as _HW1_YOURNAME.ipynb_ (for example, _HW1_FELIX.ipynb_). You are responsible for checking that you have correctly submitted the correct document. If your code cannot run, you may receive NO point. \n",
        "\n",
        "- Please justify all short answers with a brief explanation. If you use latex command in the markdown, **2 points** bonus will be awarded.   \n",
        "\n",
        "- You only use the Python packages included in the following cell. You are not allowed to use other advanced package or modules unless you are permitted to. \n",
        "\n",
        "- In your final submission include the plots produced by the unedited code as presented below, as well as any additional plots produced after editing the code during the course of a problem. You may find it necessary to copy/paste relevant code into additional cells to accomplish this.\n",
        "\n",
        "- Feel free to use the lecture notes and other resources. But you\n",
        "must understand, write, and hand in your own answers. In addition, you must write and submit\n",
        "your own code in the programming part of the assignment (we may run your code).\n",
        "If you copy someone else homework solution, both of you may receive ZERO point. \n",
        "\n",
        "\n",
        "- Colab is preferred. However, if you use Anaconda, please download the .mat file locally and save it to the same folder as this homework file. "
      ],
      "metadata": {
        "id": "OBXsMJSdwmv9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1: Theory (40pt)\n",
        "\n",
        "\n",
        "\n",
        "## Q1.1 Coin Triplets I (20pt)\n",
        "**If you cannot solve Q1.1, you may consider to use Monte Carlo Simulation to get numerical results. If your simulation is correct, you can still get most of points.** \n",
        "\n",
        "If you keep on tossing a fair coin, \n",
        "- What is the expected number of tosses such that you can have HHH (heads heads heads) in a row? \n",
        "\n",
        "- What is the expected number of tosses to have THH (tails heads heads) in a row?\n"
      ],
      "metadata": {
        "id": "Yh-6weg3wpbB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Solution:"
      ],
      "metadata": {
        "id": "ClN9yrT-w3Um"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Q1.2 (10pt)\n",
        "Suppose two distinct states $i, j$ of a Markov chain satisfy\n",
        "$$ P(T_j< T_i|X_0=i)= P(T_i < T_j | X_0=j)$$\n",
        "where $T_j=\\inf\\{n\\ge 1: X_n=j\\}$, which is the first return time to state $j$. Show that, if $X_0 = i$ the expected number of visits to $j$ prior to re-visiting $i$ is one, i.e.,\n",
        "$$\\mb{E}_i[\\sum_{n=1}^\\infty \\m{I}(X_n=j) \\m{I}(n\\le T_i)]=1 $$\n"
      ],
      "metadata": {
        "id": "F8YRodnN3zl3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Solution:"
      ],
      "metadata": {
        "id": "x3pLXb5A36c9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Q1.3: Reversiblility (10pt)\n",
        "\n",
        "A Markov chain on $S=\\{0,1,\\dots, N\\}$ has transition probabilities $P(0,0) = 1-\\lambda_0, P(i,i+1)=\\lambda_i, P(i+1, i) =\\mu_{i+1}$ for $i=0,\\dots, N-1$ and $P(N,N)=1-\\mu_N$. Show that the process\n",
        "is reversible in equilibrium.  "
      ],
      "metadata": {
        "id": "tFxZ-MxPTZFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Solution:\n"
      ],
      "metadata": {
        "id": "glsYtpkG17j6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Q2: Beam Search (20pt)\n",
        "**This problem can be used to present in showcase day as extra credits.**\n",
        "\n",
        "In class, we didn't implement the last step in Beam Search so it will output the best sequence until it hit the stop_token or reaches the maximum step_times. Note the last step is \n",
        "- In the end, we obtain the set of final candidate output sequences based on these sequences. we choose the sequence with the highest of the following score as the output sequence:\n",
        "$$\\frac{1}{L^\\alpha} \\log p(X_1, X_2, \\dots, X_L | X_0)= \\frac{1}{L^\\alpha}\\sum_{t=1}^L \\log p(X_{t}|X_{t-1}) $$\n",
        "where $L$ is the length of the final candidate sequence and $\\alpha$ is usually set to 0.75. Since a longer sequence has more logarithmic terms in the summation, the term $L^\\alpha$ in the denominator penalizes long sequences.\n",
        "\n",
        "In this question, you are going to modify ```generate_status``` function in the ```SimpleMarkovModel``` class. Then we are going to test with the same dataset in class. \n",
        "\n"
      ],
      "metadata": {
        "id": "FfCAA4JuU96b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleMarkovModel(object):\n",
        "    def __init__(self, status_num=None):\n",
        "        self.pi = np.zeros(shape=(status_num))\n",
        "        self.P  = np.zeros(shape=(status_num, status_num))\n",
        "\n",
        "    def fit(self, x):\n",
        "        if type(x[0]) == list:\n",
        "            for clist in x:\n",
        "                self.pi[clist[0]] += 1\n",
        "                for cindex in range(0, len(clist) - 1):\n",
        "                    self.P[clist[cindex ], clist[cindex + 1]] += 1\n",
        "        else:\n",
        "            for index in range(0, len(x) - 1):\n",
        "                self.pi[x[index]] += 1\n",
        "                self.P[x[index ], x[index + 1]] += 1\n",
        "        # normalization\n",
        "        self.pi = self.pi / np.sum(self.pi)\n",
        "        normalization = np.sum(self.P, axis=1) \n",
        "        normalization[normalization == 0] = 1  \n",
        "        self.P = self.P / normalization[:, np.newaxis]  \n",
        "\n",
        "\n",
        "    def generate_status(self, step_times=10, stop_status=None, set_start_status=None, beam_num=5, alpha = 0.75):\n",
        "        \"\"\"\n",
        "        full beam search\n",
        "        :param step_times: maximum step_times\n",
        "        :param stop_status: list of stoping state\n",
        "        :param set_start_status: set the initial state manually \n",
        "        :param beam_num: only when search_type=\"beam\" to keep top k\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        ### TO DO:Modify the following code to choose the sequences of step_times*beam_num with the score given above!\n",
        "\n",
        "        if stop_status is None:\n",
        "            stop_status = []\n",
        "  \n",
        "        start_status = np.random.choice(len(self.pi.reshape(-1)),\n",
        "                                        p=self.pi.reshape(-1)) if set_start_status is None else set_start_status\n",
        "    \n",
        "        rst = [start_status]\n",
        "        top_k_rst = [[start_status]]\n",
        "        top_k_prob = [0.0]\n",
        "        for _ in range(0, step_times):\n",
        "            new_top_k_rst = []\n",
        "            new_top_k_prob = []\n",
        "            for k_index, k_rst in enumerate(top_k_rst):\n",
        "                  k_rst_last_status = k_rst[-1]\n",
        "                  # get top k largest idx \n",
        "                  top_k_idx = self.P[k_rst_last_status, : ].argsort()[::-1][0:beam_num]\n",
        "                  for top_k_status in top_k_idx:\n",
        "                    new_top_k_rst.append(k_rst + [top_k_status])\n",
        "                    new_top_k_prob.append(top_k_prob[k_index] + np.log(1e-12+self.P[k_rst_last_status, top_k_status]))\n",
        "\n",
        "            # sort all beam_num*beam_num results and get the top beam_num \n",
        "            top_rst_idx = np.asarray(new_top_k_prob).argsort()[::-1][0:beam_num]\n",
        "            rst = new_top_k_rst[top_rst_idx[0]]\n",
        "            # update\n",
        "            top_k_rst = []\n",
        "            top_k_prob = []\n",
        "            for top_idx in top_rst_idx[:beam_num]:\n",
        "              if new_top_k_rst[top_idx][-1] in stop_status:\n",
        "                  rst = new_top_k_rst[top_idx]\n",
        "                  break\n",
        "              else:\n",
        "                  top_k_rst.append(new_top_k_rst[top_idx])\n",
        "                  top_k_prob.append(new_top_k_prob[top_idx])  \n",
        "\n",
        "        return rst\n"
      ],
      "metadata": {
        "id": "GHWMMuOgXteE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/probml/probml-data/main/data/timemachine.txt\"\n",
        "response = requests.get(url)\n",
        "data = response.text\n",
        "lines = [s + \"\\n\" for s in response.text.split(\"\\n\")]\n",
        "raw_dataset = [re.sub(\"[^A-Za-z]+\", \" \", st).lower().split() for st in lines]\n",
        "\n",
        "# Concat sentences into single string of chars\n",
        "# skip blank lines\n",
        "sentences = [s for s in raw_dataset if s]\n",
        "\n",
        "# Add the start and stop tokens to each sentence in the file\n",
        "START_TOKEN = '<s>'\n",
        "STOP_TOKEN = '</s>'\n",
        "sentence_list = []\n",
        "for sentence in sentences:\n",
        "  sentence_list.append([START_TOKEN] + sentence + [STOP_TOKEN])\n",
        "\n",
        "# Building the dict and the map from the words to integers for the purpose of training.\n",
        "word2idx={}\n",
        "idx2word={}\n",
        "idx=0\n",
        "for line in sentence_list:\n",
        "    for word in line:\n",
        "        if word not in word2idx:\n",
        "            word2idx[word]=idx\n",
        "            idx2word[idx]=word\n",
        "            idx+=1\n",
        "\n",
        "# Transform the words in each sentence to integers\n",
        "train_data=[]\n",
        "for line in sentence_list:\n",
        "    train_data.append([word2idx[word] for word in line])\n",
        "\n",
        "smm=SimpleMarkovModel(status_num=len(word2idx))\n",
        "smm.fit(train_data)\n",
        "\n"
      ],
      "metadata": {
        "id": "2vY2v2MtY8kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx_list=smm.generate_status(set_start_status=word2idx['you'] , stop_status=[word2idx[word] for word in ['</s>']])\n",
        "print([idx2word[idx] for idx in idx_list])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gCvwIXzZQv1",
        "outputId": "4024d041-a672-4081-9816-f037e3bd2693"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['you', 'cannot', 'move', 'freely', 'in', 'the', 'time', 'traveller', 's', 'pause', 'that']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Q3: Natural Language Processing (40pt)\n",
        "**This problem can be used to present in showcase day as extra credits.**\n",
        "\n",
        "One of applications of Markov chain in natural language processing is language model. In particular, we will work on uni-gram and bi-gram. \n",
        "\n",
        "The following data set provides you with the training data files (a subset of the One Billion Word Language Modeling Bench-\n",
        "mark). Each line in each file contains a whitespace-tokenized sentence.\n",
        "\n",
        "- 1b benchmark.train.tokens: data for training your language models.\n",
        "\n",
        "I have precessed these dataset for the purposed of this problem. The total number of unique words is 80661. In bi-gram model, the size of the transition matrix is $80661\\times 80661$. It is impossible the store these matrices directly and we have to take advantage of the sparsity of the transition matrix. "
      ],
      "metadata": {
        "id": "8tuc45hoX1IY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/yexf308/AppliedStochasticProcess/main/HW/HW2/1b_benchmark.train.tokens?raw=true -O 1b_benchmark.train.tokens\n",
        "!wget https://raw.githubusercontent.com/yexf308/AppliedStochasticProcess/main/HW/HW2/1b_benchmark.test.tokens?raw=true -O 1b_benchmark.test.tokens\n"
      ],
      "metadata": {
        "id": "mAA41pgPX9Wc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6501354-1627-4b59-800d-86b6a4543774"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-30 01:28:25--  https://raw.githubusercontent.com/yexf308/AppliedStochasticProcess/main/HW/HW2/1b_benchmark.train.tokens?raw=true\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8422714 (8.0M) [text/plain]\n",
            "Saving to: ‘1b_benchmark.train.tokens’\n",
            "\n",
            "1b_benchmark.train. 100%[===================>]   8.03M  41.3MB/s    in 0.2s    \n",
            "\n",
            "2023-03-30 01:28:26 (41.3 MB/s) - ‘1b_benchmark.train.tokens’ saved [8422714/8422714]\n",
            "\n",
            "--2023-03-30 01:28:26--  https://raw.githubusercontent.com/yexf308/AppliedStochasticProcess/main/HW/HW2/1b_benchmark.test.tokens?raw=true\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1650147 (1.6M) [text/plain]\n",
            "Saving to: ‘1b_benchmark.test.tokens’\n",
            "\n",
            "1b_benchmark.test.t 100%[===================>]   1.57M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-03-30 01:28:26 (12.5 MB/s) - ‘1b_benchmark.test.tokens’ saved [1650147/1650147]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "START_TOKEN = \"<s>\"\n",
        "STOP_TOKEN = \"</s>\"\n",
        "\n",
        "class FileParser:\n",
        "    def __init__(self, train_file=\"1b_benchmark.train.tokens\", test_file=\"1b_benchmark.test.tokens\"):\n",
        "        self.TRAIN_FILE = train_file\n",
        "        self.TEST_FILE  = test_file\n",
        "    \n",
        "    def get_train_file_tokens(self):\n",
        "        return self._tokenize(self._get_sentences(self.TRAIN_FILE))\n",
        "\n",
        "    def get_test_file_tokens(self):\n",
        "        return self._tokenize(self._get_sentences(self.TEST_FILE))    \n",
        "    \n",
        "    def get_train_file_sentence_tokens(self):\n",
        "        return self._tokenize(self._get_sentences(self.TRAIN_FILE), flatten=False)  \n",
        "\n",
        "    def get_test_file_sentence_tokens(self):\n",
        "        return self._tokenize(self._get_sentences(self.TEST_FILE), flatten=False)      \n",
        "    \n",
        "    def _flatten(self, l):\n",
        "        return [word for sublist in l for word in sublist]\n",
        "\n",
        "    def _tokenize(self, sentence_list, flatten=True):\n",
        "        tokenized_sentences = [re.split(\"\\s+\", sentence.strip()) for sentence in sentence_list]\n",
        "\n",
        "        if flatten:\n",
        "            return self._flatten(tokenized_sentences)\n",
        "        \n",
        "        return tokenized_sentences\n",
        "\n",
        "    def _get_sentences(self, file_path):\n",
        "        \n",
        "        l = []\n",
        "        with open(file_path, \"r\") as f:\n",
        "            l = f.readlines()\n",
        "        \n",
        "        # Add the start and stop tokens to each sentence in the file\n",
        "        sentence_list = []\n",
        "        for sentence in l:\n",
        "            sentence_list.append(START_TOKEN + \" \" + sentence + \" \" + STOP_TOKEN)\n",
        "        \n",
        "        return sentence_list"
      ],
      "metadata": {
        "id": "3TmWIyNRX_Hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp = FileParser()\n",
        "train_tokens = fp.get_train_file_sentence_tokens()\n",
        "test_tokens = fp.get_test_file_sentence_tokens()\n",
        "# The fourth line of training set\n",
        "print(train_tokens[3])"
      ],
      "metadata": {
        "id": "MltcEq_OYA84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd8290cb-8391-42fa-abe5-c0e33d2b3cf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', 'Abu', 'Dhabi', 'is', 'going', 'ahead', 'to', 'build', 'solar', 'city', 'and', 'no', 'pollution', 'city', '.', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the dict and the map from the words to integers for the purpose of training.\n",
        "word2idx={}\n",
        "idx2word={}\n",
        "idx=0\n",
        "for line in train_tokens:\n",
        "    for word in line:\n",
        "        if word not in word2idx:\n",
        "            word2idx[word]=idx\n",
        "            idx2word[idx]=word\n",
        "            idx+=1\n",
        "\n",
        "# Transform the words in each sentence to integers\n",
        "train_data=[]\n",
        "for line in train_tokens:\n",
        "    train_data.append([word2idx[word] for word in line])\n",
        "\n",
        "# there are 80661 unique words. \n",
        "len(word2idx)"
      ],
      "metadata": {
        "id": "E_1CmlnTYEKn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08504d34-e834-4e8d-d25d-e59367bd03b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80661"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Your task:** Complete the code in the ```LanguageModel``` class as follows. \n",
        "\n",
        "1. (10pt) Implement **Laplace Smoothing** with the smoothing factor =1 in the function ```get_unigram_probability``` and ```get_bigram_probability```. In the bigram model, try to implement it as efficient as possible. **Hint:** For bigrams $(t_1,t_2)$ which do not occur in the sample, what is $p(t_2|t_1)$? For fixed $t_1$, are these probability the same?\n",
        "\n",
        "2. (10pt) Complete functions ```get_unigram_sentence_log_probability``` and ```get_bigram_sentence_log_probability``` to calculate the log probability of the given sentence. You need to consider the situation that when Laplace Smoothing is true and the situation that when Laplace Smoothing is false. \n",
        "\n",
        "\n",
        "3. (10pt) To make your language model work better, you will implement linear interpolation smoothing between\n",
        "unigram, bigram. \n",
        "\\begin{align}\n",
        "p'(t_2|t_1) = \\lambda_1 p(t_2) + \\lambda_2 p(t_2|t_1)\n",
        "\\end{align}\n",
        "where $p'$ represents the smoothed probability, the hyperparameters $\\lambda_1, \\lambda_2$ are weights on the unigram,\n",
        "bigram language models, respectively. So $\\lambda_1+\\lambda_2= 1$. \n",
        "Complete functions ```get_linear_interpolation_probability``` and ```get_linear_interpolation_sentence_log_probability``` with $\\mm\\lambda = (\\lambda_1, \\lambda_2)$ stored in ```linear_interpolation_factors```. \n",
        "\n",
        "4. (10pt) Testing the smoothed probability with several setences in the testing dataset. You should test when laplace smoothing is True and when laplace smoothing is False. You might find some words in testing dataset are not appeared in training set and you can set a default probability for this situation.  "
      ],
      "metadata": {
        "id": "S8rUaq3SsLRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel(object):\n",
        "    def __init__(self, laplace_smoothing=False, laplace_smoothing_factor=1,linear_interpolation_factors=(0.3, 0.7)):\n",
        "        self.unigram_freqs = {}\n",
        "        self.unigram_corpus_length = 0\n",
        "        self.num_unique_unigrams = 0\n",
        "\n",
        "        self.bigram_freqs = {}\n",
        "        self.bigram_corpus_length = 0\n",
        "        self.num_unique_bigrams = 0\n",
        "\n",
        "        self.laplace_smoothing = laplace_smoothing\n",
        "        self.laplace_smoothing_factor = laplace_smoothing_factor\n",
        "        \n",
        "    def fit_unigram(self, tokens): \n",
        "        for clist in tokens: \n",
        "          for cindex in range(len(clist)):\n",
        "            t = clist[cindex]\n",
        "            self.unigram_freqs[(t)] = self.unigram_freqs.get((t),0) + 1\n",
        "\n",
        "        self.unigram_corpus_length = sum(list(self.unigram_freqs.values()))\n",
        "        self.num_unique_unigrams = len(list(self.unigram_freqs.keys()))\n",
        "\n",
        "    def fit_bigram(self, tokens):\n",
        "        \n",
        "      for clist in tokens:\n",
        "        for cindex in range(0, len(clist) - 1):\n",
        "          t1, t2 =  clist[cindex], clist[cindex+1]\n",
        "          self.bigram_freqs[(t1, t2)] = self.bigram_freqs.get((t1, t2), 0) + 1\n",
        "\n",
        "      self.bigram_corpus_length = sum(list(self.bigram_freqs.values())) \n",
        "      self.num_unique_bigrams = len(list(self.bigram_freqs.keys()))   \n",
        "\n",
        "    def get_unigram_probability(self, unigram): \n",
        "\n",
        "      if self.laplace_smoothing:\n",
        "        ### TO DO: ###\n",
        "        return\n",
        "      else:\n",
        "        prob_numerator   = self.unigram_freqs.get(unigram, 0) \n",
        "        prob_denominator = self.unigram_corpus_length\n",
        "        prob = float(prob_numerator) / float(prob_denominator)  \n",
        "\n",
        "        return prob\n",
        "\n",
        "    def get_bigram_probability(self, bigram):\n",
        "      t1, t2 = bigram \n",
        "\n",
        "      if self.laplace_smoothing:\n",
        "        ### TO DO: ###\n",
        "        return\n",
        "      else:  \n",
        "        prob_numerator   = self.bigram_freqs.get((t1, t2), 0)\n",
        "        prob_denominator = self.unigram_freqs.get(t1, 0) \n",
        "        if prob_denominator == 0: \n",
        "            print(f\"Error: 'get_bigram_probability()' has a denominator of 0 for {bigram}\")\n",
        "            return float('inf') \n",
        "        \n",
        "        prob = float(prob_numerator) / float(prob_denominator)\n",
        "        return prob \n",
        "\n",
        "    def get_linear_interpolation_probability(self, bigram):\n",
        "      ### TO DO: ###\n",
        "      return    \n",
        "\n",
        "    def get_unigram_sentence_log_probability(self, sentence):\n",
        "      ### TO DO: ###\n",
        "      return \n",
        "    \n",
        "    def get_bigram_sentence_log_probability(self, sentence):\n",
        "      ### TO DO: ###\n",
        "      return \n",
        "\n",
        "    def get_linear_interpolation_sentence_log_probability(self, sentence):  \n",
        "      ### TO DO: ###\n",
        "      return\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R07oXHe4tQPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code starts here. \n",
        "LM =LanguageModel()\n",
        "LM.fit_bigram(train_data)\n",
        "LM.fit_unigram(train_data)\n",
        "LM.bigram_corpus_length"
      ],
      "metadata": {
        "id": "XI-c6AAwBlHQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87747a0f-7972-48f0-ced3-aa5c7ad32807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1622905"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LM.bigram_freqs.get((0, 1), 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lr6B3E7MJCgS",
        "outputId": "f5b0931d-d163-472c-e4d2-4b761da9b1bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LM.unigram_freqs.get(0, 0) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoVSf699L9lp",
        "outputId": "e47c3dd6-22cc-4e97-a262-dc414d4f7035"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "61530"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LM.get_bigram_probability((0,1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3g7NG_XcMXzA",
        "outputId": "84a27f57-6b5c-47a2-a4ef-afc57fd00bc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0006988460913375589"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    }
  ]
}