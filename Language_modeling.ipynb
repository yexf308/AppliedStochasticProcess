{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOi4Y2O3FZnQChOhvZvuVBn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yexf308/AppliedStochasticProcess/blob/main/Language_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Language modeling\n",
        "$\\def\\m#1{\\mathbf{#1}}$\n",
        "$\\def\\mm#1{\\boldsymbol{#1}}$\n",
        "$\\def\\mb#1{\\mathbb{#1}}$\n",
        "$\\def\\mr#1{\\mathrm{#1}}$\n",
        "$\\def\\c#1{\\mathcal{#1}}$\n",
        "$\\newenvironment{rmat}{\\left[\\begin{array}{rrrrrrrrrrrrr}}{\\end{array}\\right]}$\n",
        "$\\newcommand\\brm{\\begin{rmat}}$\n",
        "$\\newcommand\\erm{\\end{rmat}}$\n",
        "$\\newenvironment{cmat}{\\left[\\begin{array}{ccccccccc}}{\\end{array}\\right]}$\n",
        "$\\newcommand\\bcm{\\begin{cmat}}$\n",
        "$\\newcommand\\ecm{\\end{cmat}}$\n",
        "\n",
        "\n",
        "One important application of Markov models is to create language models (LM), which are models\n",
        "which can generate (or score) a sequence of words.\n",
        "\n",
        "## How to score a sentence?\n",
        "For example, the sentence \"I like to eat apple.\" is treated as list ['I', 'like', 'to', 'eat', 'apple']. Then the probability to score a sentence becomes to find the probability of the list \n",
        "$$ P(\\text{I like to eat apple}) = P(\\text{'I', 'like', 'to', 'eat', 'apple'})$$\n",
        "\n",
        "We need to collect lots of text data from news, article, or even wikipedia. The number of vocabulary in daily use is roughtly 5000 words. \n",
        "\n",
        "- We need $5000^n$ text data to sufficiently calculate the probability for the sentence of length n. $n=2,3,4,...$\n",
        "\n",
        "- If you randomly generate a sentence, the probability that sentence makes sense is extremely small or even 0. \n",
        "\n",
        "- So to judge a sentence makes sense, we only need a relative probability. It is sufficient as far as our model can say the following (or similar) statement.\n",
        "$$P(\\text{'I', 'like', 'to', 'eat', 'apple'}) >P(\\text{'apple', 'likes', 'to', 'eat', 'me'}) $$\n",
        "\n",
        "- Instead of studying the whole sequence, we can focus on local connection. For example, it is more likely to find 'eat apple'\n",
        "than 'apple eats', i.e.,\n",
        "$$ P(\\text{apple}|\\text{eat}) > P(\\text{eat}|\\text{apple})$$\n",
        "\n",
        "\n",
        "- When we use a finite-state Markov model with\n",
        "a memory of length $m=n-1$. it is called an $n$-gram model. \n",
        "\n",
        "   - If $m = 1$, we get a unigram model (no dependence on previous words);\n",
        "\n",
        "   - If $m = 2$, we get a bigram model (depends\n",
        "on previous word); \n",
        "\n",
        "   - If $m = 3$, we get a trigram model (depends on previous two words);\n",
        "\n",
        "- These days, most LMs are built using recurrent neural nets, which have\n",
        "unbounded memory. However, simple n-gram models can still do quite well when trained with enough data.\n",
        "\n",
        "\n",
        "### Example\n",
        "The sentence is 'I saw the red house'.  $\\langle s\\rangle$ is the starting of the sentence and $\\langle /s\\rangle$ is rhe ending of the sentence.\n",
        "\n",
        "- In bigram model, \n",
        "\\begin{align}\n",
        "& P(\\text{I, saw, the, red, house}) \\\\\n",
        "\\approx {} & P(\\text{I}\\mid\\langle s\\rangle) P(\\text{saw}\\mid \\text{I}) P(\\text{the}\\mid\\text{saw}) P(\\text{red}\\mid\\text{the}) P(\\text{house}\\mid\\text{red}) P(\\langle /s\\rangle\\mid \\text{house})\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "- In trigram model,\n",
        "\\begin{align}\n",
        "& P(\\text{I, saw, the, red, house}) \\\\\n",
        "\\approx {} & P(\\text{I}\\mid \\langle s\\rangle,\\langle s\\rangle) P(\\text{saw}\\mid\\langle s\\rangle,I) P(\\text{the}\\mid\\text{I, saw}) P(\\text{red}\\mid\\text{saw, the}) P(\\text{house}\\mid\\text{the, red}) P(\\langle /s\\rangle\\mid\\text{red, house})\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RnyJcySS8DI3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuJBHbg17-Et",
        "outputId": "9da3e3f3-7d6c-4947-ba87-e804e1d756f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ],
      "source": [
        "%pylab inline \n",
        "import re\n",
        "import string\n",
        "import requests\n",
        "import collections\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleMarkovModel(object):\n",
        "    def __init__(self, status_num=None):\n",
        "        # initial probability vector\n",
        "        self.pi = np.zeros(shape=(status_num))\n",
        "        # transition probability matrix\n",
        "        self.P = np.zeros(shape=(status_num, status_num))\n",
        "\n",
        "    def fit(self, x):\n",
        "        \"\"\"\n",
        "        Based on training data, calculate initial probability vector and transition probability matrix\n",
        "        param x: x can be a single list or list of list. [s1, s2, ..., sn] or [[s11,s12,...,s1m],[s21,s22,...,s2n],...]\n",
        "        The difference is in calculating initial probability vector. In single list, we can use all states to inference the initial\n",
        "        prob vector. In list of list, we will use the initial states of sub-list to inference.\n",
        "        return:\n",
        "        \"\"\"\n",
        "        if type(x[0]) == list:\n",
        "            for clist in x:\n",
        "                self.pi[clist[0]] += 1\n",
        "                for cindex in range(0, len(clist) - 1):\n",
        "                    self.P[clist[cindex ], clist[cindex + 1]] += 1\n",
        "        else:\n",
        "            for index in range(0, len(x) - 1):\n",
        "                self.pi[x[index]] += 1\n",
        "                self.P[x[index ], x[index + 1]] += 1\n",
        "        # normalization\n",
        "        self.pi = self.pi / np.sum(self.pi)\n",
        "        normalization = np.sum(self.P, axis=1) # <--\n",
        "        normalization[normalization == 0] = 1  # <--\n",
        "        self.P = self.P / normalization[:, np.newaxis]  \n",
        "\n",
        "\n",
        "    def predict_log_joint_prob(self, status_list, set_init_prob=None):\n",
        "        \"\"\"\n",
        "        calculate the log of the joint probability\n",
        "        param: status_list:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        prob = self.pi if set_init_prob is None else set_init_prob\n",
        "\n",
        "        log_prob = np.log(prob[status_list[0]])\n",
        "        for index in range(0, len(status_list) - 1):\n",
        "            log_prob += np.log(self.P[status_list[index], status_list[index + 1]])\n",
        "        return log_prob  \n",
        "\n",
        "\n",
        "    def predict_prob_distribution(self, time_steps=None, set_init_prob=None, set_prob_trans_matrix=None):\n",
        "        \"\"\"\n",
        "        calculate the prob distribution after time_steps.\n",
        "        Allow set_init_prob and set_prob_trans_matrix to set initial prob and prob_trans_matrix\n",
        "        :param time_steps:\n",
        "        :param set_init_prob:\n",
        "        :param set_prob_trans_matrix:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        prob = self.pi if set_init_prob is None else set_init_prob\n",
        "        trans_matrix = self.P if set_prob_trans_matrix is None else set_prob_trans_matrix\n",
        "        for _ in range(0, time_steps):\n",
        "            prob = np.matmul(prob, trans_matrix)\n",
        "        return prob  \n",
        "\n",
        "    def predict_next_step_prob_distribution(self, current_status=None):\n",
        "        \"\"\"\n",
        "        predict next step probility distribution\n",
        "        :param current_status:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        return self.P[[current_status], :]  \n",
        "\n",
        "    def predict_next_step_status(self, current_status=None):\n",
        "        \"\"\"\n",
        "        predict the most probable state in the next step\n",
        "        :param current_status:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        return np.argmax(self.predict_next_step_prob_distribution(current_status))   \n",
        "\n",
        "\n",
        "    def generate_status(self, step_times=10, stop_status=None, set_start_status=None, search_type=\"greedy\", beam_num=5):\n",
        "        \"\"\"\n",
        "        greedy search and beam search\n",
        "        :param step_times: maximum step_times\n",
        "        :param stop_status: list of stoping state\n",
        "        :param set_start_status: set the initial state manually \n",
        "        :param search_type: greedy and beam\n",
        "        :param beam_num: only when search_type=\"beam\" to keep top k\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if stop_status is None:\n",
        "            stop_status = []\n",
        "  \n",
        "        start_status = np.random.choice(len(self.pi.reshape(-1)),\n",
        "                                        p=self.pi.reshape(-1)) if set_start_status is None else set_start_status\n",
        "        if search_type == \"greedy\":\n",
        "       \n",
        "            rst = [start_status]\n",
        "            for _ in range(0, step_times):\n",
        "                next_status = self.predict_next_step_status(current_status=start_status)\n",
        "                rst.append(next_status)\n",
        "                if next_status in stop_status:\n",
        "                    break   \n",
        "                start_status = next_status  \n",
        "\n",
        "        else:\n",
        "            # beam search\n",
        "            rst = [start_status]\n",
        "            top_k_rst = [[start_status]]\n",
        "            top_k_prob = [0.0]\n",
        "            for _ in range(0, step_times):\n",
        "                new_top_k_rst = []\n",
        "                new_top_k_prob = []\n",
        "                for k_index, k_rst in enumerate(top_k_rst):\n",
        "                    k_rst_last_status = k_rst[-1]\n",
        "                    # get top k largest idx \n",
        "                    top_k_idx = self.P[k_rst_last_status, : ].argsort()[::-1][0:beam_num]\n",
        "                    for top_k_status in top_k_idx:\n",
        "                        new_top_k_rst.append(k_rst + [top_k_status])\n",
        "                        new_top_k_prob.append(top_k_prob[k_index] + np.log(1e-12+self.P[k_rst_last_status, top_k_status]))\n",
        "                # sort all beam_num*beam_num results and get the top beam_num \n",
        "                top_rst_idx = np.asarray(new_top_k_prob).argsort()[::-1][0:beam_num]\n",
        "                rst = new_top_k_rst[top_rst_idx[0]]\n",
        "                # update\n",
        "                top_k_rst = []\n",
        "                top_k_prob = []\n",
        "                for top_idx in top_rst_idx[:beam_num]:\n",
        "                    if new_top_k_rst[top_idx][-1] in stop_status:\n",
        "                        rst = new_top_k_rst[top_idx]\n",
        "                        break\n",
        "                    else:\n",
        "                        top_k_rst.append(new_top_k_rst[top_idx])\n",
        "                        top_k_prob.append(new_top_k_prob[top_idx])        \n",
        "\n",
        "        return rst\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_S339ierNIOg"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/probml/probml-data/main/data/timemachine.txt\"\n",
        "response = requests.get(url)\n",
        "data = response.text\n",
        "lines = [s + \"\\n\" for s in response.text.split(\"\\n\")]\n",
        "raw_dataset = [re.sub(\"[^A-Za-z]+\", \" \", st).lower().split() for st in lines]\n",
        "\n",
        "# Print first few lines\n",
        "for sentence in raw_dataset[:10]:\n",
        "    print(sentence)\n",
        "\n",
        "# Concat sentences into single string of chars\n",
        "# skip blank lines\n",
        "sentences = [s for s in raw_dataset if s]\n",
        "\n",
        "# Add the start and stop tokens to each sentence in the file\n",
        "START_TOKEN = '<s>'\n",
        "STOP_TOKEN = '</s>'\n",
        "sentence_list = []\n",
        "for sentence in sentences:\n",
        "  sentence_list.append([START_TOKEN] + sentence + [STOP_TOKEN])\n",
        "\n",
        "\n",
        "for sentence in sentence_list[:5]:\n",
        "    print(sentence)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwDl6DufNgFj",
        "outputId": "feea7e55-8321-46e5-a355-7f8fc9ec3a07"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
            "[]\n",
            "['i']\n",
            "[]\n",
            "['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him', 'was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and', 'twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the', 'fire', 'burned', 'brightly', 'and', 'the', 'soft', 'radiance', 'of', 'the', 'incandescent', 'lights', 'in', 'the', 'lilies', 'of', 'silver', 'caught', 'the', 'bubbles', 'that', 'flashed', 'and', 'passed', 'in', 'our', 'glasses', 'our', 'chairs', 'being', 'his', 'patents', 'embraced', 'and', 'caressed', 'us', 'rather', 'than', 'submitted', 'to', 'be', 'sat', 'upon', 'and', 'there', 'was', 'that', 'luxurious', 'after', 'dinner', 'atmosphere', 'when', 'thought', 'roams', 'gracefully', 'free', 'of', 'the', 'trammels', 'of', 'precision', 'and', 'he', 'put', 'it', 'to', 'us', 'in', 'this', 'way', 'marking', 'the', 'points', 'with', 'a', 'lean', 'forefinger', 'as', 'we', 'sat', 'and', 'lazily', 'admired', 'his', 'earnestness', 'over', 'this', 'new', 'paradox', 'as', 'we', 'thought', 'it', 'and', 'his', 'fecundity']\n",
            "[]\n",
            "['you', 'must', 'follow', 'me', 'carefully', 'i', 'shall', 'have', 'to', 'controvert', 'one', 'or', 'two', 'ideas', 'that', 'are', 'almost', 'universally', 'accepted', 'the', 'geometry', 'for', 'instance', 'they', 'taught', 'you', 'at', 'school', 'is', 'founded', 'on', 'a', 'misconception']\n",
            "[]\n",
            "['is', 'not', 'that', 'rather', 'a', 'large', 'thing', 'to', 'expect', 'us', 'to', 'begin', 'upon', 'said', 'filby', 'an', 'argumentative', 'person', 'with', 'red', 'hair']\n",
            "[]\n",
            "['<s>', 'the', 'time', 'machine', 'by', 'h', 'g', 'wells', '</s>']\n",
            "['<s>', 'i', '</s>']\n",
            "['<s>', 'the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him', 'was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and', 'twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the', 'fire', 'burned', 'brightly', 'and', 'the', 'soft', 'radiance', 'of', 'the', 'incandescent', 'lights', 'in', 'the', 'lilies', 'of', 'silver', 'caught', 'the', 'bubbles', 'that', 'flashed', 'and', 'passed', 'in', 'our', 'glasses', 'our', 'chairs', 'being', 'his', 'patents', 'embraced', 'and', 'caressed', 'us', 'rather', 'than', 'submitted', 'to', 'be', 'sat', 'upon', 'and', 'there', 'was', 'that', 'luxurious', 'after', 'dinner', 'atmosphere', 'when', 'thought', 'roams', 'gracefully', 'free', 'of', 'the', 'trammels', 'of', 'precision', 'and', 'he', 'put', 'it', 'to', 'us', 'in', 'this', 'way', 'marking', 'the', 'points', 'with', 'a', 'lean', 'forefinger', 'as', 'we', 'sat', 'and', 'lazily', 'admired', 'his', 'earnestness', 'over', 'this', 'new', 'paradox', 'as', 'we', 'thought', 'it', 'and', 'his', 'fecundity', '</s>']\n",
            "['<s>', 'you', 'must', 'follow', 'me', 'carefully', 'i', 'shall', 'have', 'to', 'controvert', 'one', 'or', 'two', 'ideas', 'that', 'are', 'almost', 'universally', 'accepted', 'the', 'geometry', 'for', 'instance', 'they', 'taught', 'you', 'at', 'school', 'is', 'founded', 'on', 'a', 'misconception', '</s>']\n",
            "['<s>', 'is', 'not', 'that', 'rather', 'a', 'large', 'thing', 'to', 'expect', 'us', 'to', 'begin', 'upon', 'said', 'filby', 'an', 'argumentative', 'person', 'with', 'red', 'hair', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the dict and the map from the words to integers for the purpose of training.\n",
        "word2idx={}\n",
        "idx2word={}\n",
        "idx=0\n",
        "for line in sentence_list:\n",
        "    for word in line:\n",
        "        if word not in word2idx:\n",
        "            word2idx[word]=idx\n",
        "            idx2word[idx]=word\n",
        "            idx+=1"
      ],
      "metadata": {
        "id": "pprfBkELP5Ju"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# there are 4579 unique words. \n",
        "len(word2idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4AaU9HvP9Zm",
        "outputId": "63153e8e-8a85-43ae-d2e3-e8a42898a8a4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4581"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the words in each sentence to integers\n",
        "train_data=[]\n",
        "for line in sentence_list:\n",
        "    train_data.append([word2idx[word] for word in line])"
      ],
      "metadata": {
        "id": "3Fbc0u5UP7dH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smm=SimpleMarkovModel(status_num=len(word2idx))\n",
        "smm.fit(train_data)"
      ],
      "metadata": {
        "id": "Dr8VnVcCR46J"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's see how many entries are 0.\n",
        "np.sum(smm.P==0)/(smm.P.shape[0]*smm.P.shape[1])\n",
        "# this is really sparse matrix!!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkWNuZT1ecjM",
        "outputId": "9ff9d256-d2f5-45ad-88f7-339a847d0082"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9990304762403064"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unigrams\n",
        "# top five most common words in unigrams\n",
        "\n",
        "from collections import Counter\n",
        "sentence_list_flatten = list(np.concatenate(sentence_list).flat)\n",
        "top_5_word_counts = Counter(sentence_list_flatten).most_common(5)\n",
        "print(top_5_word_counts)"
      ],
      "metadata": {
        "id": "7kt8anCuaviR",
        "outputId": "d22c59ba-5558-42e0-cf09-7d8f164fb973",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('the', 2261), ('i', 1267), ('and', 1245), ('of', 1155), ('a', 816)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# top five common words in stationary distribution\n",
        "p= smm.predict_prob_distribution(time_steps=50)\n",
        "idxs = np.argpartition(p, -5)[-5:].tolist()\n",
        "for idx in idxs:\n",
        "  print(idx2word[idx])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKn0uPIJbrIV",
        "outputId": "6a2f3b4d-64b5-43c5-ae54-fe3bcf74426a"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a\n",
            "of\n",
            "and\n",
            "the\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# give a default small probability for zero probability entry.\n",
        "smm.P=np.where(smm.P==0,1.0/smm.P.shape[0],smm.P)\n",
        "\n",
        "# let's try some random sentences\n",
        "print(smm.predict_log_joint_prob([word2idx[word] for word in [\"time\",\"machine\",\"is\",\"good\"]], set_init_prob=p))\n",
        "print(smm.predict_log_joint_prob([word2idx[word] for word in [\"time\",\"good\",\"is\",\"machine\"]], set_init_prob=p))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzlxoR2lgBiA",
        "outputId": "d9d06a26-9313-43f7-8452-36f7e2ba874c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-24.0859057346767\n",
            "-30.880822608145053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate text data\n",
        "Markov chain is a generative model so it can be used to generate text. There are two methods: greedy search and beam search.\n",
        "### Greedy Search\n",
        "It is the greedy algorithm, so at each step we will go to the state that achieves the maximum probability, i.e.,\n",
        "\n",
        "$$\n",
        "S_{\\text{next}}^*=\\arg\\max_{S_{\\text{next}}}p(X_{t}=S_{\\text{next}}\\mid X_{t-1}=S_{\\text{current}})\n",
        "$$   \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xjTmotxx32q-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx_list=smm.generate_status(search_type=\"greedy\",set_start_status=word2idx['you'] , stop_status=[word2idx[word] for word in ['</s>']])"
      ],
      "metadata": {
        "id": "TzW9nAB76K45"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print([idx2word[idx] for idx in idx_list])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okEbZQtd64vd",
        "outputId": "65e43746-a2be-420e-803f-96d64b0a2c86"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['you', 'know', 'how', 'it', 'was', 'a', 'little', 'people', 'were', 'no', 'doubt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beam Search\n",
        "With Greedy Search, we took just the single best word at each position and we considered each position in isolation. Once we had identified the best word for that position, we did not examine what came before it (ie. in the previous position), or after it. So it may not find the optimal sequence.\n",
        "\n",
        "If the goal is to obtain the most likely sequence, we may consider using exhaustive search: exhaustively enumerate all the possible output sequences with their conditional probabilities, and then output the one that scores the highest predicted probability. That will be too expensive!!\n",
        "\n",
        "Beam search is a compromise between greedy search and exhaustive search.  \n",
        "\n",
        "**Hyperparameter**:  the beam size $k$.\n",
        "\n",
        "**Steps**:\n",
        "\n",
        "-  At time step 1, we select the $k$ tokens with the highest predicted probabilities. Each of them will be the first token of $k$ candidate output sequences, respectively. \n",
        "\n",
        "- At each subsequent time step, based on the $k$ candidate output sequences at the previous time step, we continue to select $k$ candidate output sequences with the highest predicted probabilities from $k|S|$ possible choices.\n",
        "\n",
        "- In the end, we obtain the set of final candidate output sequences based on these sequences. we choose the sequence with the highest of the following score as the output sequence:\n",
        "$$\\frac{1}{L^\\alpha} \\log p(X_1, X_2, \\dots, X_L | X_0)= \\frac{1}{L^\\alpha}\\sum_{t=1}^L \\log p(X_{t}|X_{t-1}) $$\n",
        "where $L$ is the length of the final candidate sequence and $\\alpha$ is usually set to 0.75. Since a longer sequence has more logarithmic terms in the summation, the term $L^\\alpha$ in the denominator penalizes long sequences.\n",
        "\n",
        "\n",
        "The computational cost of beam search is $O(k|S|L)$. This result is in between that of greedy search and that of exhaustive search. Greedy search can be treated as a special case of beam search arising when the beam size is set to 1.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Example\n",
        "The process of beam search (beam size: 2, maximum length of an output sequence: 3).\n",
        "\n",
        "<img src=\"https://github.com/yexf308/AppliedStochasticProcess/blob/main/image/beam.png?raw=true\" width=\"800\" />\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0q6zfYp7fs4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx_list=smm.generate_status(search_type=\"beam\",set_start_status=word2idx['you'] , stop_status=[word2idx[word] for word in ['</s>']])"
      ],
      "metadata": {
        "id": "c3oaR0rt8-5c"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print([idx2word[idx] for idx in idx_list])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yM3UuP99HmN",
        "outputId": "72d74be7-8dbd-4829-9585-6cf072a7dd2c"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['you', 'cannot', 'move', 'freely', 'in', 'the', 'time', 'traveller', 's', 'pause', 'that']\n"
          ]
        }
      ]
    }
  ]
}