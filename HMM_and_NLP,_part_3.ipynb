{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQKZ3P6cj1nmccyvW3V7qX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yexf308/AppliedStochasticProcess/blob/main/HMM_and_NLP%2C_part_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.special import logsumexp\n",
        "import math\n",
        "from typing import List, Tuple\n",
        "from numpy.typing import ArrayLike\n",
        "import numpy.typing as npt"
      ],
      "metadata": {
        "id": "pEwEHGY44E6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title  data loader class\n",
        "\n",
        "class IdentityDict(dict):\n",
        "    \"\"\"This special dictionary can map a number to itself, and is used in the \n",
        "    mappings of hidden states in the simulated data.\n",
        "    \"\"\"\n",
        "    def __missing__(self, key):\n",
        "        return key\n",
        "\n",
        "class DataLoader(object):\n",
        "    \"\"\"Data loader for HMM models to infer/learn.\n",
        "\n",
        "    This class provides list of observed sequences w/o the corresponding hidden \n",
        "    states. All strings will be UTF-8 encoded. The data has two kinds of \n",
        "    sources: 1) simulation; or 2) real world task Part-of-Speech (POS) tagging\n",
        "    whose further description can be found at TODO. Format of the two types of\n",
        "    data are illustrated as follows.\n",
        "    \n",
        "    Case 1: simulated data\n",
        "        * observation: a list of integers, e.g. [1, 2, 3, 4, 0]. \n",
        "        * hidden: a list of integers, e.g. [0, 1, 2, 3, 4]. \n",
        "\n",
        "    Case 2: TODO\n",
        "\n",
        "    Attributes:\n",
        "        method: \n",
        "            a string indicating the source of data, could be \"simulate\" or\n",
        "            \"real\"\n",
        "        include_hiddens: \n",
        "            a boolen variable indicating whether hidden states will be returned \n",
        "            along with the observed sequences. \n",
        "        data_path: \n",
        "            a string indicting the path to the real world data set.\n",
        "        initial: \n",
        "            a numpy array to specify the distribution over initial states, whose\n",
        "            shape should be (num_hiddens,)\n",
        "        transition: \n",
        "            a numpy array to specify the transition probability between\n",
        "            hidden states, whose shape should be (num_hiddens, num_hiddens)\n",
        "        emission: \n",
        "            a numpy array to specify the emission probability from hidden\n",
        "            states to observations, whose shape should be \n",
        "            (num_observations, num_hiddents)\n",
        "        D:\n",
        "            an integer to fix the length of simulated sequences.\n",
        "        num_hiddens:\n",
        "            an integer, the number of different hiddent states.\n",
        "        num_observations:\n",
        "            an integer, the number of possible observations.\n",
        "    Methods:\n",
        "        TODO\n",
        "        TODO:\n",
        "            return the marginal probs of the generated sequences\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, method:str='simulate', \n",
        "                include_hiddens:bool=False, \n",
        "                data_path:str=None,\n",
        "                initial:npt.ArrayLike=None,\n",
        "                transition:npt.ArrayLike=None,\n",
        "                emission:npt.ArrayLike=None,\n",
        "                D:int=None\n",
        "                ) -> None:\n",
        "        \"\"\"Initialise the necessary parameters of DataLoader.\n",
        "        \n",
        "        This method initialises all necessary attributes of the class in order \n",
        "        to simulate data or read POS tagging data set.\n",
        "        \n",
        "        Args:\n",
        "            meanings of arguments are illustrated in the comments to class\n",
        "            attributes.\n",
        "            \n",
        "            Note that there are two mutually exclusive scenarios:\n",
        "             1. method is 'simulate':\n",
        "                initial, transition, and emission all need to be specified in \n",
        "                order to simulate data\n",
        "             2. method is 'pos':\n",
        "                data_path needs to be specified in order to read data\n",
        "            \n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        # check the value of method\n",
        "        assert method in ['simulate', 'pos'], \\\n",
        "            \"Unknown source of data: {0}; please input either \\'simulate\\' or \\\n",
        "                \\'pos\\'.\".format(method)\n",
        "        self.method = method\n",
        "        self.include_hiddens = include_hiddens\n",
        "        \n",
        "        if method == 'simulate':\n",
        "            assert (initial is not None) and (transition is not None) and \\\n",
        "                (emission is not None), \\\n",
        "                \"Please check the specified \\'initial\\', \\'transition\\', and\" \\\n",
        "                    + \" \\'emission\\'.\"\n",
        "\n",
        "            assert (initial.shape[0] == transition.shape[1]) and \\\n",
        "                (transition.shape[0] == transition.shape[1]) and \\\n",
        "                    (transition.shape[0] == emission.shape[1]), \\\n",
        "                \"Please check the dimensions of  \\'initial\\', \\'transition\\',\"\\\n",
        "                    + \" and \\'emission\\'.\"\n",
        "        \n",
        "            self.initial = initial\n",
        "            self.transition = transition\n",
        "            self.emission = emission\n",
        "            self.D = D\n",
        "            \n",
        "            self.num_hiddens = self.transition.shape[0]\n",
        "            self.num_observations = self.emission.shape[0]\n",
        "            \n",
        "        else:\n",
        "            assert data_path is not None, \\\n",
        "                \"Please specify the path to the POS tagging dataset.\"\n",
        "            \n",
        "            self.data_path = data_path\n",
        "            # TODO: complete the pos tagging data set loader.\n",
        "            \n",
        "        \n",
        "        self._construct_idx2str()\n",
        "    \n",
        "    def _construct_idx2str(self) -> None:\n",
        "        \"\"\"Setup the mappings from indices to hiddens/observations.\n",
        "        \n",
        "        This method constructs the mappings from the integers to the capital\n",
        "        alphabets that will appear in the observations.\n",
        "        \n",
        "        Args: \n",
        "            None\n",
        "            \n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        \n",
        "        if self.method == 'simulate':\n",
        "            self.hidden_dict = IdentityDict()\n",
        "            self.observation_dict = IdentityDict()\n",
        "        else:\n",
        "            # TODO\n",
        "            # create the dictionaries used in real data set\n",
        "            pass\n",
        "        \n",
        "    def _generate_data(self):\n",
        "        \"\"\"Generate a sequence of hidden states and the corresponding\n",
        "        observations.\n",
        "        \n",
        "        This method will first sample a sequence of hiddent states following\n",
        "        the Markov chain specified by self.initial and self.transmition. Then\n",
        "        it will sample observations for each hidden state following \n",
        "        self.emission.\n",
        "        \n",
        "        Args:\n",
        "            None\n",
        "            \n",
        "        Returns:\n",
        "            A **list** consists of the following two elements:\n",
        "            observations:\n",
        "                a list of integers\n",
        "            hiddens:\n",
        "                a list of integers\n",
        "        \"\"\"\n",
        "        \n",
        "        hidden_states = []\n",
        "        observations = []\n",
        "\n",
        "        end = 0\n",
        "        last_transition = self.initial\n",
        "        step = 0\n",
        "        \n",
        "        while not end:\n",
        "            # sample current hidden state\n",
        "            cur_h = np.random.choice(self.num_hiddens, 1, p=last_transition)[0]\n",
        "            # sample current observation\n",
        "            cur_o = np.random.choice(self.num_observations, 1,\n",
        "                                     p=self.emission[:, cur_h])[0]\n",
        "            \n",
        "            # add the sampled hidden state and observation to the lists\n",
        "            hidden_states.append(cur_h)\n",
        "            observations.append(cur_o)\n",
        "            \n",
        "            step += 1\n",
        "            # sample if the next hiddent state is the end of sequence\n",
        "            end = (cur_o == 0) if self.D is None else int(step >= self.D)\n",
        "            last_transition = self.transition[cur_h, :]\n",
        "            \n",
        "        hidden_states = [self.hidden_dict[x] for x in hidden_states]\n",
        "        observations = [self.observation_dict[x] for x in observations]\n",
        "        \n",
        "        return observations, hidden_states\n",
        "    \n",
        "    def _read_postag_data(self):\n",
        "        pass\n",
        "    \n",
        "    def get_true_params(self) -> \\\n",
        "        Tuple[npt.ArrayLike, npt.ArrayLike, npt.ArrayLike]:\n",
        "        \"\"\"Return a tuple consists of the true parameters of data generation.\n",
        "        \n",
        "        This method returns a tuple consisits of the three parameters of the HMM\n",
        "        that genreates the data list.\n",
        "        \n",
        "        Args:\n",
        "            None\n",
        "            \n",
        "        Returns:\n",
        "            self.initial: \n",
        "                an numpy.array which specifies the distribution over initial\n",
        "                states.\n",
        "            self.transition: \n",
        "                an numpy.array which specifies the transition probability between hidden states.\n",
        "            self.emission: \n",
        "                an numpy.array which specifies the emission probability from hidden states to observations.\n",
        "            self.end:\n",
        "                an numpy.array which specifies the transition probability from \n",
        "                hidden states to end of sequence.\n",
        "        \"\"\"\n",
        "        if self.method == 'pos':\n",
        "            print(\"Oops, we don't know the true parameters of the real-world \\\n",
        "                dataset.\")\n",
        "            return None\n",
        "        else:\n",
        "            return self.initial, self.transition, self.emission, self.end\n",
        "    \n",
        "    def get_data_list(self, n_samples:int=None):\n",
        "        \"\"\"Return a list of sequences as sample for HMM's inference/learning.\n",
        "        \n",
        "        This method returns a list of (lists of integers) to be the inputs for \n",
        "        the inference/learning modules of HMM. There will be two kinds of lists\n",
        "        according to different sources of data:\n",
        "        1) 'simulate': \n",
        "            * observation: a list of integers, e.g. [0, 1, 2, 1, 3];\n",
        "            * hidden: a list of integers, e.g. [0, 1, 2, 1, 3].\n",
        "        2) 'pos': in this case, each string would be like TODO\n",
        "        \n",
        "        Args:\n",
        "            n_samples:\n",
        "                Optional, an integer to specify the number of samples in the \n",
        "                simulated data list. \n",
        "                self.method should be 'simulate' if this argument is given.\n",
        "        \n",
        "        Returns:\n",
        "            data_list:\n",
        "                a list consists of \n",
        "        \"\"\"\n",
        "        \n",
        "        data_list = []\n",
        "        \n",
        "        if self.method == 'simulate':\n",
        "            assert type(n_samples) == int, \\\n",
        "                \"Please specify the number of sample you want to generate.\"\n",
        "            for _ in range(n_samples):\n",
        "                if self.include_hiddens:\n",
        "                    data_list.append(self._generate_data())\n",
        "                else:\n",
        "                    data_list.append(self._generate_data()[0])\n",
        "        else:\n",
        "            pass # TODO\n",
        "            \n",
        "        return data_list\n",
        "    "
      ],
      "metadata": {
        "cellView": "form",
        "id": "EFZzIrJ34u-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference for Hidden Markov Models\n",
        "This notebook is referred from Michael Gutmann's notes.\n",
        "\n",
        "In this notebook, we will implement an unsupervised learning method for a hidden Markov model (HMM).\n",
        "\n",
        "In this notebook, as before, we work with a homogeneous HMM with discrete observed and hidden states with the necessary (conditional) probability distributions represented by (conditional) probability tables.\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "When we are learning a new language, we have to learn, among many other things, the grammar of the language as well as its vocabulary. In the `HMM basics` notebook, we have introduced latent sentence templates that we can use to construct sentences, e.g. Subject -> Verb -> Object. The latent sentence templates were given by a hidden Markov chain. We can think that this hidden Markov chain, i.e. the latent variables together with the transition distribution, corresponds to a simple model of the grammar of the language. In turn, proper use of the vocabulary means knowing when one can use a given word. We can think that the emission distribution corresponds to a simple model of the proper use of the vocabulary.\n",
        "\n",
        "In a supervised setting, e.g. when learning a language from a textbook, we may have annotations of the hidden states together with the used words. For example\n",
        " - sentences: `['I like dogs', 'I really love cats']`;\n",
        " - annotations: `[[Subject, Verb, Object], [Subject, Adverb, Verb, Object]]`.\n",
        " \n",
        "When we have observations on the hidden variables, the learning problem is simpler: we can use standard maximum likelihood estimation for discrete random variables as with directed acyclic graphs. We have seen that this corresponds to frequency counting of words and annotations, and their co-occurences. Hence, given data on the state of the hidden variables, we could simply count the frequency of \n",
        "1. the states $k$ of the first hidden variable $h_1$ as $a_k$, \n",
        "2. transitions from hidden state $k'$ to $k$ as $A_{k',k}$, and\n",
        "3. emitting word $m$ from hidden state $k$ as $B_{m,k}$.\n",
        "\n",
        "Counting would thus allow us to learn the parameters of the HMM.\n",
        "\n",
        "But, what if we do not have such annotations? The languages that we speak emerged naturally through complex social interactions and not from a concrete set of hidden grammar rules. So is it possible for computers to learn the grammar and use of vocabulary automatically without supervision? Yes, with some limitations, it can be done using HMMs and unsupervised learning.\n",
        "\n",
        "As explained in the lecture, we can iteratively learn the parameters of an HMM model solely from observed sentences via the Baum-Welch algorithm, which consists of two steps: \n",
        "1. *Expectation*-step (E-step), where the expected log-likelihood is computed on filled-in data using the current parameters of the model and the inference algorithms implemented in the previous notebooks,\n",
        "2. *Maximisation*-step (M-step), where updated parameters are computed such that they maximise the expectation from the E-step.\n",
        "\n",
        "Before we implement the Baum-Welch algorithm, we also have to represent the parametrisation of the HMM in terms of variables in computer code which we do next."
      ],
      "metadata": {
        "id": "CdbRpbHouq5e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parametrisation of discrete HMM\n",
        "\n",
        "Following the slides in the lecture and the example in the `HMM basics` notebook, we model the homogeneous discrete HMM model with the following three parameters:\n",
        "1. `_initial_`: the distribution over initial hidden states, which is implemented as a vector with shape `(num_hiddens,)`. It corresponds to the vector $\\mathbf{a}\\in\\mathbb{R}^K$ introduced in the lecture where $a_k=p(h_1=k;\\mathbf{a})$.\n",
        "2. `_transition_`: the transition probability between the hidden states, which is implemented as a matrix with shape `(num_hiddens, num_hiddens)`. It corresponds to the matrix $\\mathbf{A}\\in\\mathbb{R}^{K\\times K}$ introduced in the lecture. In this notebook, we follow the convention $A_{k',k}=p(h_i=k | h_{i-1}=k'; \\mathbf{A})$ for ease of implementation, which is the *transpose* of the matrix in the slides.\n",
        "3. `_emission_`: the emission probability from a hidden state to an observation, which is implemented as a matrix with shape `(num_observations, num_states)`. It corresponds to the vector $\\mathbf{B}\\in\\mathbb{R}^{M\\times K}$ introduced in the lecture where $B_{m,k}=p(v_i=m|h_i=k; \\mathbf{B})$.\n",
        "\n",
        "In this tutorial, all of the above parameters will be stored as tensors, i.e. `np.array`, and the dimensions will be given in the corresponding sections."
      ],
      "metadata": {
        "id": "6Gq132ddverg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title HMM class\n",
        "\"\"\"\n",
        "Implementation of HMMs\n",
        "\n",
        "We take the following view:\n",
        "* a  [probabilistic model]  =        a  [class]\n",
        "* an [inference operation]  =        a  [public member function]\n",
        "*    [inference operation]  includes    [partition / marginal / argmax / sampling]\n",
        "* an [inference algorithm]  =        a  [private member function] \n",
        "\n",
        "For the HMM model, we have the following correspondance:\n",
        "  inference operation          inference algorithm\n",
        "* partition             <--->  forward\n",
        "* marginal              <--->  forward-backward\n",
        "* max                   <--->  viterbi\n",
        "* argmax                <--->  viterbi-backtracking\n",
        "* sampling              <--->  ancestral sampling\n",
        "* conditional sampling  <--->  forward-filtering backward-sampling\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "class HMM(object):\n",
        "    def __init__(self, initial, transition, emission):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            initial: size=[num_state]\n",
        "            transition: size=[num_state, num_state] from state -> to state\n",
        "            emission: size=[num_observation, num_state]\n",
        "        \"\"\"\n",
        "        self.initial = initial\n",
        "        self.transition = transition\n",
        "        self.emission = emission\n",
        "        self.num_state = transition.shape[0]\n",
        "\n",
        "        # assume the end state is the last state\n",
        "        # self.end_state = self.num_state - 1\n",
        "        return \n",
        "\n",
        "    def _forward(self, x):\n",
        "        \"\"\"Forward algorithm for computing the alpha and the partition,\n",
        "        implemented in the log space\n",
        "\n",
        "        Args:\n",
        "          x: size=[max_len]\n",
        "\n",
        "        Returns:\n",
        "          alpha: size=[max_len, num_state]\n",
        "          Z: float\n",
        "        \"\"\"\n",
        "        T = len(x)\n",
        "        N = self.num_state\n",
        "\n",
        "        alpha = np.zeros((T, N)) \n",
        "        alpha[0] = self.emission[x[0]] + self.initial\n",
        "        for t in range(1, T):\n",
        "            emission_t = self.emission[x[t]]\n",
        "            alpha[t] = logsumexp(alpha[t - 1].reshape(N, 1) +\n",
        "                                 self.transition + \n",
        "                                 emission_t.reshape(1, N)\n",
        "                                 , axis=0)\n",
        "\n",
        "        Z = logsumexp(alpha[T - 1], axis=0)\n",
        "        return alpha, Z\n",
        "\n",
        "    def _backward(self, x):\n",
        "        \"\"\"Backward algorithm for computing the beta and the marginals,\n",
        "        implemented in the log space,\n",
        "        could be replaced by automatic differentiation\n",
        "\n",
        "        Args:\n",
        "          x: size=[max_len]\n",
        "\n",
        "        Returns:\n",
        "          beta: size=[max_len, num_state]\n",
        "        \"\"\"\n",
        "        T = len(x)\n",
        "        N = self.num_state\n",
        "\n",
        "        beta = np.zeros((T, N))\n",
        "\n",
        "        # t = [T - 2, T - 3 ,..., 0]\n",
        "        for t in range(T - 2, -1, -1):\n",
        "            emission_t_1 = self.emission[x[t + 1]]\n",
        "            beta[t] = logsumexp(beta[t + 1].reshape(1, N) + \n",
        "                                self.transition + \n",
        "                                emission_t_1.reshape(1, N), \n",
        "                                axis=1)\n",
        "        return beta\n",
        "\n",
        "    def _viterbi(self, x):\n",
        "        \"\"\"Viterbi algorithm with back-tracking for computing the most probable\n",
        "        latent state sequence.\n",
        "\n",
        "        Args:\n",
        "          x: size=[max_len]\n",
        "\n",
        "        Returns:\n",
        "          max_z: size=[max_len]\n",
        "          max_p: float\n",
        "        \"\"\"\n",
        "        T = len(x)\n",
        "        N = self.num_state\n",
        "\n",
        "        max_s = np.zeros((T, N))\n",
        "        max_ptr = np.zeros((T, N)) # look up table \n",
        "        max_s[0] = self.initial + self.emission[x[0]]\n",
        "        for t in range(1, T):\n",
        "            emission_t = self.emission[x[t]]\n",
        "            log_phi_t = self.transition + emission_t.reshape(1, N)\n",
        "            max_s[t] = np.max(max_s[t-1].reshape(N, 1) + log_phi_t, axis=0)\n",
        "            max_ptr[t] = np.argmax(max_s[t-1].reshape(N, 1) + log_phi_t, axis=0)\n",
        "        \n",
        "        # max_p = np.max(max_s[T - 1] + self.transition[:, self.end_state])\n",
        "        max_p = np.max(max_s[T - 1])\n",
        "\n",
        "        # backtracking\n",
        "        max_z = np.zeros(T).astype(int)\n",
        "        # max_z[T - 1] = np.argmax(max_s[T - 1] + self.transition[:, self.end_state])\n",
        "        max_z[T - 1] = np.argmax(max_s[T - 1])\n",
        "        for t in range(T - 2, -1, -1):\n",
        "            # print(t + 1)\n",
        "            # print(max_z[t + 1])\n",
        "            max_z[t] = max_ptr[t + 1, max_z[t + 1]]\n",
        "        return max_z, max_p\n",
        "\n",
        "    def partition(self, x):\n",
        "        \"\"\"Log partition the of HMM\n",
        "\n",
        "        Args:\n",
        "          x: size=[max_len]\n",
        "\n",
        "        Returns:\n",
        "          log_z: float\n",
        "        \"\"\"\n",
        "        _, log_z = self._forward(x)\n",
        "        return log_z\n",
        "\n",
        "    def marginal(self, x):\n",
        "        \"\"\"Marginal distribution of the latent sequences given the observation x\n",
        "\n",
        "        Args:\n",
        "          x: size=[max_len]\n",
        "\n",
        "        Returns:\n",
        "          node_marginal, size=[max_len, num_states] # p(h_t | x)\n",
        "          edge_marginal, size=[max_len - 1, num_states, num_states] p(h_t, h_{t + 1} | x)\n",
        "        \"\"\"\n",
        "        alpha, log_px = self._forward(x)\n",
        "        beta = self._backward(x)\n",
        "        node_marginal = alpha + beta - log_px\n",
        "\n",
        "        T = alpha.shape[0]\n",
        "        N = alpha.shape[1]\n",
        "        emission = self.emission[x]  # size = [T, num_state]\n",
        "\n",
        "        # log edge marginal probability at step t from state i to state j is a T * N * N tensor\n",
        "        # log p(t, i, j) = alpha(t, i) + transition(i, j) + emission(t + 1, j) + beta(t, j) - Z\n",
        "        if(T >= 2):\n",
        "            edge_marginal = alpha[:-1].reshape(T - 1, N, 1) +\\\n",
        "                            self.transition.reshape(1, N, N) +\\\n",
        "                            emission[1:].reshape(T - 1, 1, N) +\\\n",
        "                            beta[1:].reshape(T - 1, 1, N) - log_px\n",
        "        else: edge_marginal = None\n",
        "        return alpha, beta, log_px, node_marginal, edge_marginal\n",
        "\n",
        "    def argmax(self, x):\n",
        "        \"\"\"Most probable latent sequence given the observation x\n",
        "        \n",
        "        Args:\n",
        "          x: size=[max_len]\n",
        "\n",
        "        Returns:\n",
        "          marginal: size=[batch, max_len, num_state]\n",
        "        \"\"\"\n",
        "        max_z, max_log_prob = self._viterbi(x)\n",
        "        return max_z, max_log_prob\n",
        "\n",
        "    def log_prob(self, x):\n",
        "        \"\"\"Log probability of a given pair of observed x and latent z\n",
        "        \n",
        "        Args:\n",
        "          x: size=[max_len]\n",
        "\n",
        "        Returns:\n",
        "          log_prob: size=[batch]\n",
        "        \"\"\"\n",
        "        _, log_px = self._forward(x)\n",
        "        return log_px\n",
        "\n",
        "    def get_delta_param(self, initial, transition, emission):\n",
        "        assert (initial.shape == self.initial.shape) and \\\n",
        "            (transition.shape == self.transition.shape) and \\\n",
        "            (emission.shape == self.emission.shape), \"\"\"\n",
        "            Please make sure the inputs have the same shape to the current\\\n",
        "                parameters.\n",
        "            \"\"\"\n",
        "            \n",
        "        params_current = np.vstack([self.initial, self.transition, self.emission]).flatten()\n",
        "        params_new = np.vstack([initial, transition, emission]).flatten()\n",
        "\n",
        "        return np.linalg.norm(params_new - params_current, ord=2)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KjOgFcMmvqvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview of the implementation\n",
        "\n",
        "We implement the unsupervised learning algorithm in an object-oriented manner. We here define the \n",
        "`HMMOptimiser` class and provide an overview of its attributes and methods.\n",
        "\n",
        "The *attributes* of this class are:\n",
        " 1. `model`: the HMM model whose parameters we would like to fit. The `HMM` model class is implemented includes the inference operations implemented in the previous notebooks.\n",
        " 2. `num_hiddens`: an integer that specifies the number of possible hidden states. Since we do not know the *real* number of hidden states in the unsupervised setting, this is a tunable hyper-parameter in the [Baum-Welch algorithm](https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm).\n",
        " 3. `num_observations`: an integer that specifies the number of possible observed states. To handle the situations where the test data may contain observed states that do not appear in the training data, we can set this number to more than all the observed states in the training data.\n",
        "\n",
        "The *methods* of the  `HMMOptimiser` class are:\n",
        "1. `__init__`: initialises the optimiser with the provided arguments.\n",
        "2. `baum_welch`: implements the Baum-Welch algorithm, which iteratively invokes the following two methods to update the parameters:\n",
        "    1. `_e_step`: implements the expectation step by using the `marginal` method from the `HMM` class.\n",
        "    2. `_m_step`: implements the maximisation step.\n",
        "    3. `_initial_params_`: initialises the parameters.\n",
        "    4. `_stop_criterion_`: checks if the iterations should be stopped.\n",
        "\n",
        "\n",
        "In the following sections we explain how to implement the above methods in order to learn the parameters of an HMM model from data.\n",
        "\n"
      ],
      "metadata": {
        "id": "lUj-XqZdwtfD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation\n",
        "### E-step\n",
        "\n",
        "The E-step of the Baum-Welch algorithm computes the ***expected* complete-data log-likelihood** given the observed data $\\mathcal{D}$ and the current parameter estimate $\\theta_{\\text{old}}$.\n",
        "Although we do not have the complete data, we can **probabilistically fill-in the latents** using the HMM model with current parameters $\\theta_{\\text{old}}$.\n",
        "\n",
        "For the discrete-valued HMM in this notebook, we compute the EM objective derived in the lecture, i.e. \n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "J(\\theta, \\theta_{\\text{old}}) =\n",
        "& \\sum_{j=1}^{N} \\sum_{k=1}^{K} p(h_1=k|\\mathcal{D}_j; \\theta_{\\text{old}})\\log a_k\\\\\n",
        "&+ \\sum_{j=1}^{N} \\sum_{i=2}^{d_j} \\sum_{k=1}^{K} \\sum_{k'=1}^{K} p(h_i=k, h_{i-1}=k'|\\mathcal{D}_j; \\theta_{\\text{old}})\\log A_{k',k}\\\\\n",
        "&+ \\sum_{j=1}^{N} \\sum_{i=1}^{d_j} \\sum_{k=1}^{K} \\sum_{m=1}^{M} \\mathbb{I}(v_i^{(j)}=m)p(h_i=k|\\mathcal{D}_j, \\theta_{old})\\log B_{m,k}\n",
        "\\end{aligned}\n",
        "$$\n",
        "where $N$ is the number of sequences, $K$ is the number of possible hidden states, $d_j$ is the length of the $j$-th sequence, $M$ is the number of observed states (e.g. words), and $\\theta_{\\text{old}} = (\\log a_k, \\log A_{k',k}, \\log B_{m,k})$ are the current parameters.\n",
        "\n",
        "To compute the above objective in the E-step we must know the following distributions:\n",
        " - $p(h_1|\\mathcal{D}_j; \\theta_{\\text{old}})$,\n",
        " - $p(h_i, h_{i-1}|\\mathcal{D}_j; \\theta_{\\text{old}})$,\n",
        " - $p(h_i|\\mathcal{D}_j, \\theta_{old})$.\n",
        "\n",
        "You should recognise these quantities from the previous notebooks: the first and last are the quantities calculated in the inference notebook, i.e. $p(h_i \\mid v_{1:d})$, and the second quantity is the joint marginal of neighbouring states $p(h_i, h_{i-1} \\mid v_{1:d})$ that can be computed as \n",
        "\n",
        "$$\n",
        "p(h_i, h_{i-1} \\mid v_{1:d}) = \\frac{\\alpha_{i-1}(h_{i-1}) \\beta_{i}(h_{i}) p(h_{i} \\mid h_{i-1}) p(v_i \\mid h_i)}{p(v_{1:d})}\n",
        "$$\n",
        "\n",
        "This shows that the learning algorithm is based on the inference methods discussed before.\n",
        "The code that computes the required quantities using the forward-backward method is implemented in the `HMM` model class.\n",
        "\n",
        "\n",
        "Some useful information to understand the above code:\n",
        " 1.  `hk_list` stores the values of $p(h_i = k| \\mathcal{D}_j; \\theta_{old}), \\forall k\\in[1,\\dots,K], \\forall i\\in[1,\\dots,d_j], \\forall j\\in[1,\\dots, N]$; thus each element is an `np.array` with size [$d_j$, $K$].\n",
        " 2.  `hkk_list` stores the values of $p(h_i=k, h_{i-1}=k' | D_j; \\theta_{old}), \\forall k,k'\\in[1,\\dots,K], \\forall i\\in[2,\\dots,d_j], \\ \\forall j\\in[1,\\dots,N]$, thus each element is an `np.array` with size [$d_j$, $K$, $K$]."
      ],
      "metadata": {
        "id": "iJsiEbQTyIpH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### M-step\n",
        "\n",
        "The M-step of the Baum-Welch algorithm maximises the expected log-likelihood after filling-in the unobserved data. For the discrete-valued HMM parametrised using probability tables we can obtain the following closed-form solutions to the optimisation problem:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "a_k & = \\frac{1}{n}\\sum_{j=1}^{n}p(h_1=k|\\mathcal{D}_j;\\theta_{\\text{old}}); \\\\\n",
        "A_{k', k} & = \\frac{\\sum_{j=1}^{n}\\sum_{i=2}^{d_j}p(h_i=k,h_{i-1}=k'|\\mathcal{D}_j;\\theta_{\\text{old}})}{\\sum_k\\sum_{j=1}^{n}\\sum_{i=2}^{d_j}p(h_i=k,h_{i-1}=k'|\\mathcal{D}_j;\\theta_{\\text{old}})} \\\\\n",
        "B_{m,k} & = \\frac{\\sum_{j=1}^{n}\\sum_{i=1}^{d_j}\\mathbb{I}(v_i^{(j)}=m)p(h_i=k|\\mathcal{D}_j;\\theta_{\\text{old}})}{\\sum_{m}\\sum_{j=1}^{n}\\sum_{i=1}^{d_j}\\mathbb{I}(v_i^{(j)}=m)p(h_i=k|\\mathcal{D}_j;\\theta_{\\text{old}})}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "As you can see in the above equations, we enumerate through all the hidden/observed states in all sequences.\n",
        "Thus, the above update bears similarity to the supervised learning case discussed above, i.e. it is similar to counting the frequency of 1) the first hidden state $h_1$, 2) transitioning from a hidden state $k'$ to state $k$, and 3) emitting symbol $m$ from hidden state $k$. However, crucially it uses the model with the current parameters to infer the probabilities over the unknown hidden states, rather than using known observations.\n",
        "\n",
        "### Other parts\n",
        "We have discussed and implemented the two main iterative steps in the Baum-Welch algorithm. We must now also address the following essential implementation questions:\n",
        "\n",
        "- Q1. *initialisation of parameters*: how should we initialise parameters, i.e. $\\mathbf{a},\\mathbf{A},\\mathbf{B}$?\n",
        "- Q2. *stop criterion*: when should we stop the iteration of E/M-steps?\n",
        "\n",
        "\n",
        "### Initialisation of parameters\n",
        "\n",
        "Let us first consider the term $p(h_i=k|\\mathcal{D}_j;\\theta_{\\text{old}})$, which can be computed using the alpha-beta recursion $p(h_i=k|\\mathcal{D}_j;\\theta_{\\text{old}}) = \\frac{1}{Z}\\alpha_i(h_i = k)\\beta_i(h_i = k)$.  \n",
        "You should recall that \n",
        "$$\n",
        "\\alpha_i(h_i = k) = p(v_{1:i}, h_{i} = k) = \\sum_{k'} \\alpha_{i-1}(k')p(h_i = k \\mid h_{i - 1} = k')p(v_i \\mid h_i = k), \\quad \\text{with} \\quad \\alpha_1(h_1 = k) = p(h_1 = k)p(v_1 \\mid h_1 = k)\n",
        "$$ \n",
        "and \n",
        "$$\\beta_i(h_i = k) = p(v_{i+1:T} \\mid h_{i} = k) = \\sum_{k'} \\beta_{i+1}(k') p(h_{i+1} = k' \\mid h_{i} = k) p(v_{i+1} \\mid h_{i+1}=k'), \\quad \\text{with} \\quad \\beta_T(h_i = k) = 1.\n",
        "$$\n",
        "\n",
        "Now note that $\\alpha_1(k)$ is a constant for all $k$ since both $p(h_1 = k) = a_k$ and $p(v_1 = m \\mid h_1 = k) = B_{m, k}$ are constant for all $k$ and $m$. Similarly, by recursion $\\alpha_i(k)$ for all $i$ are constant, since $p(h_i = k \\mid h_{i - 1} = k') = A_{k', k}$ is constant for all $k$ and $k'$. The same argument applies to $\\beta_i(k)$ for all $i$, which is constant for all $k$. Because $\\alpha_i(k)$ and $\\beta_i(k)$ are constant for all $k$, $p(h_i=k|\\mathcal{D}_j;\\theta_{\\text{old}})$ is also a constant uniform distribution. \n",
        "A similar argument can be made to note that $p(h_i=k, h_{i-1}=k'|\\mathcal{D}_j;\\theta_{\\text{old}})$ would also be constant for all $k$ and $k'$.\n",
        "\n",
        "\n",
        "Given that all inferred distributions ($p(h_i=k|\\mathcal{D}_j;\\theta_{\\text{old}})$ and $p(h_i=k, h_{i-1}=k'|\\mathcal{D}_j;\\theta_{\\text{old}})$) would be uniform, looking at the update equations of $\\mathbf{a}$ and $\\mathbf{A}$ in the M-step you should note that the updated parameters $\\mathbf{a}$ and $\\mathbf{A}$ will remain uniform and no learning is happening. \n",
        "Moreover, because $p(h_i=k|\\mathcal{D}_j;\\theta_{\\text{old}})$ is constant, these terms in the numerator and denominator of the update for $\\mathbf{B}$ will cancel, and hence \n",
        "$$\n",
        "B_{m,k} = \\frac{\\sum_{j=1}^{n}\\sum_{i=1}^{d}\\mathbb{I}(v_i^{(j)}=m)}{\\sum_{m}\\sum_{j=1}^{n}\\sum_{i=1}^{d}\\mathbb{I}(v_i^{(j)}=m)} = \\frac{\\text{# }m\\text{ words in the dataset}}{\\text{total # words in the dataset}}.\n",
        "$$\n",
        "In particular $\\mathbf{B}_m$ vector is independent of the hidden states $k$, i.e. $B_{m, k} = B_{m, k'}$ for $\\forall k, k'$. It hence corresponds to a Markov model of order 0.\n",
        "\n",
        "\n",
        "### Stop criterion\n",
        "\n",
        "Having implemented the parameter initialisation method and the E/M-steps above, we can now run the loop of the Baum-Welch algorithm.\n",
        "However there is still one more remaining question: when should we stop iterating?\n",
        "\n",
        "There are some heuristic criteria:\n",
        " 1. *small change of parameters*: if a local maxima is reached, then the derivative of the parameters would be 0. Therefore, there should be no change of parameters in the M-step either, and hence we can stop the iterations.\n",
        " 2. *small change of likelihood*: alternatively, we can stop the iteration if the change of (log-)likelihood of the model given the data becomes small. Note that the log-likelihood is available as a by-product from the computation of the marginal and joint of the latents that are needed in the E-step.\n",
        " 3. *number of steps*: sometimes our initialisation might be particularly bad, then it may take a very long time for the learning to converge (in terms of the above criteria). To avoid excessive number of iterations we can set a maximum number of iterations and stop the loop immediately if the maximum number of iterations is exceeded.\n",
        " 4. *mix*: in practice a mixture of the above criteria is most often used.\n",
        " \n",
        "Below we ask you to fill in the code for the stopping criterion. The algorithm should stop when the maximum number of steps is exceeded, the change in parameters (`delta_param`) is less than $10^{-16}$, or the change in the log-likelihood (`delta_logpx`) is less than $10^{-8}$.\n",
        "\n",
        "### Main loop of the Baum-Welch algorithm\n",
        "\n",
        "We now have all the steps of the Baum-Welch algorithm implemented. We must now run them in the following order:\n",
        " 1. initialise the parameters of HMM;\n",
        " 2. compute the E-step with the current parameters;\n",
        " 3. update parameters with the M-step;\n",
        " 4. continue back to Step 2 if the stop criterion is not met.\n",
        "\n",
        "\n",
        "### Putting it all together\n",
        "\n",
        "We have implemented all the necessary methods of the Baum-Welch and we can finally run it on some data.\n",
        "We have provided a `DataLoader` class  to make it easy to load data or generate sequences following a hidden Markov structure.\n",
        "\n",
        "Below, we the data loader to synthesise a dataset of 300 random sequences by specifying the `initial`, `transition`, and `emission` matrices and then optimise the parameters of an `HMM` on them. We will then inspect if the learnt parameters are *close* to the true ones.\n"
      ],
      "metadata": {
        "id": "w96oi-BgymTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title HMM Optimiser class\n",
        "\n",
        "class HMMOptimiser(object):\n",
        "    \"\"\"Optimiser for the parameters of HMM.\n",
        "\n",
        "    This class implements the algorithms to optimise the parameters of HMM \n",
        "    model in discrete case, with either supervised or unsupervised data. \n",
        "\n",
        "    Attributes:\n",
        "        model: \n",
        "            an HMM model \n",
        "        supervised: \n",
        "            a boolean variable that indicates the HMM model is trained in\n",
        "            supervised or unsupervised fashion\n",
        "        num_hiddens:\n",
        "            an integer which is the number of possible hidden states\n",
        "        num_observations:\n",
        "            an integer which is the number of possible observed states\n",
        "    \n",
        "    Methods:\n",
        "        __init__:\n",
        "            initialise the class\n",
        "        fit:\n",
        "            fit the parameters of an HMM model on a given data set\n",
        "        baum_welch:\n",
        "            fit on unsupervised data\n",
        "        _initial_params:\n",
        "            initialise the parameters of HMM model in the Baum-Welch algorithm\n",
        "        _e_step:\n",
        "            estimation step of the Baum-Welch algorithm\n",
        "        _m_step:\n",
        "            maximisation step of the Baum-Welch algorithm\n",
        "        _stop_criterion:\n",
        "            criterion for stopping the iteration in the Baum-Welch algorithm\n",
        "        counts:\n",
        "            fit on supervised data\n",
        "        get_trained_model:\n",
        "            get an HMM class instance with parameters fit on a given data set\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, \n",
        "                supervised:bool=False,\n",
        "                num_hiddens:int=None,\n",
        "                num_observations:int=None\n",
        "                ) -> None:\n",
        "        super().__init__()\n",
        "        assert num_hiddens is not None and num_observations is not None\n",
        "        self.model = None\n",
        "        self.supervised = supervised\n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.num_observations = num_observations\n",
        "        \n",
        "    def fit(self, data_loader:List) -> None:\n",
        "        \"\"\"Fit the parameters on the given data loader.\n",
        "        \n",
        "        This method will fit the three parameters of HMM model on the given \n",
        "        sequences, i.e. *initial* (distribution), *transition* probability \n",
        "        matrix (between hidden states), and *emission* probability matrix (from\n",
        "        hidden states to observed states).\n",
        "        \n",
        "        There are two different types of learning:\n",
        "        1) supervised, where true hidden states are also provided in the data\n",
        "            loader;\n",
        "        2) unsupervised, where only sequences of observed states are provided in\n",
        "            the data loader.\n",
        "            \n",
        "        Note that this method doesn't return the optimised model, instead users\n",
        "        need to get it through the method \"get_trained_model\".\n",
        "            \n",
        "        Args:\n",
        "            data_loader:\n",
        "                a list whose element should be \n",
        "                    1) a tuple containing two lists of integers, if learning is\n",
        "                        supervised;\n",
        "                    2) a list of integers, if learning is unsupervised\n",
        "        \n",
        "        Return:\n",
        "            None\n",
        "        \"\"\"\n",
        "        \n",
        "        if self.supervised:\n",
        "            assert type(data_loader[0]) == tuple and len(data_loader[0]) == 2, \\\n",
        "                \"\"\"Samples in the data list should contain observations and \n",
        "                hiddens at the same time if we want to train the model in a\n",
        "                supervised way.\"\"\"\n",
        "                \n",
        "            self.counts(data_loader)\n",
        "        else:\n",
        "            self.baum_welch(data_loader)\n",
        "    \n",
        "    def _e_step(self, data_loader:complex) -> Tuple[float, List, List]:\n",
        "        \"\"\"E-step of the Baum-Welch algorithm\n",
        "        \n",
        "        This method implements the E-step which estimates the probability \n",
        "        distribution over the hidden states given a sequence of observations, \n",
        "        i.e. the following two quantities:\n",
        "            1. $p(h_i, h_{i-1} | D_j; \\theta_{old})$;\n",
        "            2. $p(h_i | D_j; \\theta_{old})$.\n",
        "        Since both of the above values have been calculated in the `marginal` \n",
        "        method of the HMM model in `hmm.py`, we can directly get the results\n",
        "        from it.\n",
        "        \n",
        "        Args:\n",
        "            data_loader:\n",
        "                a list of lists whose elements are integers.\n",
        "        \n",
        "        Return:\n",
        "            loglikelihood:\n",
        "                float whose value equals to the average loglikelihood of the \n",
        "                input data.\n",
        "            hk_list:\n",
        "                a list of np.arrays correspondding to the value of \n",
        "                $p(h_i | D_j; \\theta_{old})$\n",
        "            hkk_list:\n",
        "                a list of np.arrays correspondding to the value of \n",
        "                $p(h_i, h_{i-1} | D_j; \\theta_{old})$\n",
        "\n",
        "        \"\"\"\n",
        "        \n",
        "        hk_list = []\n",
        "        hkk_list = []\n",
        "        log_ps = []\n",
        "        \n",
        "        for o_seq in data_loader:\n",
        "            \n",
        "            _, _, log_p, hk, hkk = self.model.marginal(o_seq) \n",
        "            hk_list.append(hk)\n",
        "            hkk_list.append(hkk)\n",
        "            log_ps.append(log_p)\n",
        "            \n",
        "        return np.mean(log_ps), hk_list, hkk_list\n",
        "    \n",
        "    def _m_step(self, \n",
        "                data_loader:object, \n",
        "                hk_list:List, \n",
        "                hkk_list:List\n",
        "                ) -> Tuple[ArrayLike, ArrayLike, ArrayLike]:\n",
        "        \"\"\" M-step of the Baum-Welch algorithm\n",
        "        \n",
        "        This method implements the M-step which maximises the parameters of the\n",
        "        HMM model (self.model) given observations and the following estimations:\n",
        "            1. $p(h_i, h_{i-1} | D_j; \\theta_{old})$;\n",
        "            2. $p(h_i | D_j; \\theta_{old})$.\n",
        "        The equations used for updating the parameters are given as following:\n",
        "            1. $a_k = \\frac{1}{n}\\sum_{j=1}^{n}p(h_1=k|\\mathcal{D}_j;\\theta_{\\text{old}})$;\n",
        "            2. $A_{k,k'} = \\frac{\\sum_{j=1}^{n}\\sum_{i=2}^{d}p(h_i=k,h_{i-1}=k'|\\mathcal{D}_j;\\theta_{\\text{old}})}{\\sum_k\\sum_{j=1}^{n}\\sum_{i=2}^{d}p(h_i=k,h_{i-1}=k'|\\mathcal{D}_j;\\theta_{\\text{old}})}$\n",
        "            3. $B_{m,k} = \\frac{\\sum_{j=1}^{n}\\sum_{i=1}^{d}\\mathbb{I}(v_i^{(j)}=m)p(h_i=k|\\mathcal{D}_j;\\theta_{\\text{old}})}{\\sum_{m}\\sum_{j=1}^{n}\\sum_{i=1}^{d}\\mathbb{I}(v_i^{(j)}=m)p(h_i=k|\\mathcal{D}_j;\\theta_{\\text{old}})}$\n",
        "        \n",
        "        Args: \n",
        "            data_loader:\n",
        "                a list of lists whose elements are integers.\n",
        "            hk_list:\n",
        "                a list of np.arrays correspondding to the value of \n",
        "                $p(h_i | D_j; \\theta_{old})$\n",
        "            hkk_list:\n",
        "                a list of np.arrays correspondding to the value of \n",
        "                $p(h_i, h_{i-1} | D_j; \\theta_{old})$\n",
        "            \n",
        "        Returns:\n",
        "            _initial_:\n",
        "                1D array with shape (self.num_hiddens,) which specifies the \n",
        "                distribution over the initial hidden states\n",
        "            _transition_:\n",
        "                2D array with shape (self.num_hiddens, self.num_hiddens) where\n",
        "                cell (j,k) specifies the following probability:\n",
        "                    $p(h_{t-1}=j, h_{t}=k | D_j; \\theta_{old})$\n",
        "            _emission_:\n",
        "                2D array with shape (self.num_observations, self.num_hiddens) \n",
        "                where cell (j,k) specifies the following probability:\n",
        "                    $p(v_i=j | h_i=k, D_j; \\theta_{old})$\n",
        "        \"\"\"\n",
        "        \n",
        "        _initial_ = np.zeros(self.num_hiddens)\n",
        "        _transition_ = np.zeros([self.num_hiddens, self.num_hiddens]) \n",
        "        _emission_ = np.zeros([self.num_observations, self.num_hiddens])\n",
        "        \n",
        "        for j, obs in enumerate(data_loader):\n",
        "            # Retrieve the distributions inferred in the E-step for the current observation obs\n",
        "            hk = hk_list[j]\n",
        "            hkk = hkk_list[j]\n",
        "            # Handle obs of length 1 for which hkk is None\n",
        "            hkk = hkk if hkk is not None else float('-inf')*np.ones([1, self.num_hiddens, self.num_hiddens])\n",
        "\n",
        "            \n",
        "            _initial_ += np.exp(hk[0])\n",
        "            _transition_ += np.exp(hkk).sum(axis=0)\n",
        "            for m, ob in enumerate(obs):\n",
        "                _emission_[ob] += np.exp(hk[m])\n",
        "                #hint: not _emission_[obs] += np.exp(hk)\n",
        "\n",
        "        # Normalise the distributions\n",
        "        _initial_ /= len(data_loader)\n",
        "        _transition_ /= _transition_.sum(axis=1, keepdims=True)\n",
        "        _emission_ /= _emission_.sum(axis=0, keepdims=True)\n",
        "        \n",
        "        return _initial_, _transition_, _emission_\n",
        "        \n",
        "    @staticmethod\n",
        "    def _stop_criterion(step:int=0, \n",
        "                        delta_param:float=1e-3,\n",
        "                        delta_logpx:float=1e-1\n",
        "                        ) -> bool:\n",
        "        \"\"\" Criterion for stopping the iteration in Baum-Welch algorithm\n",
        "\n",
        "        This method keeps tracking the number of steps and change of parameters/\n",
        "        loglikelihood in order to check if the stop criterion has been \n",
        "        satisfied. If so, return True such that the Baum-Welch algorithm could\n",
        "        stop. Otherwise, return False such that the Baum-Welch algorithm could\n",
        "        keep going.\n",
        "        \n",
        "        Args:\n",
        "            step:\n",
        "                an integer indicating the index of the last iteration\n",
        "            delta_param:\n",
        "                a float indicating the change of parameters during the last\n",
        "                iteration\n",
        "            delta_logpx: \n",
        "                a float indicating the change of log-likelihood of the data\n",
        "                during the last iteration\n",
        "        \n",
        "        Returns:\n",
        "            bool:\n",
        "                True for stopping the iteration, False for keeping it going.\n",
        "        \"\"\"\n",
        "        max_steps = 100\n",
        "        min_delta_param = 1e-16\n",
        "        min_delta_logpx = 1e-8\n",
        "        stop_condition = (step >= max_steps \n",
        "                        or delta_param < min_delta_param \n",
        "                        or delta_logpx < min_delta_logpx)\n",
        "        return stop_condition\n",
        "        \n",
        "    def _initial_params(self) -> Tuple[ArrayLike, ArrayLike, ArrayLike]:\n",
        "        \"\"\"Initialisation of the parameters\n",
        "        \n",
        "        This method would return the parameters for initialising a discrete \n",
        "        HMM model. \n",
        "        \"\"\"\n",
        "        initial = np.random.uniform(size=self.num_hiddens)\n",
        "        initial /= initial.sum(axis=0)\n",
        "        \n",
        "        transition = np.random.uniform(size=[self.num_hiddens, \n",
        "                                             self.num_hiddens])\n",
        "        transition /= transition.sum(axis=1, keepdims=True)\n",
        "        \n",
        "        emission = np.random.uniform(size=[self.num_observations, \n",
        "                                           self.num_hiddens])\n",
        "        emission /= emission.sum(axis=0, keepdims=True)\n",
        "        \n",
        "        # Try initialise initial/transition to constants and randomly initialise emission, see what would happen\n",
        "        \n",
        "        return initial, transition, emission\n",
        "    \n",
        "    def baum_welch(self, data_loader:List[List[int]], verbose:bool=True):\n",
        "        \"\"\"Unsupervised learning method of discrete HMM model, i.e. Baum-Welch algorithm.\n",
        "        \n",
        "        This method would fit the parameters of an HMM model in an unsupervised \n",
        "        fashion. The overall procedure has been illustrated in the lecture and\n",
        "        the HMM Learning notebook.\n",
        "        \n",
        "        In the following implementation, the overall framework has been \n",
        "        provided. The each step has also been commented below.\n",
        "        \n",
        "        Args:\n",
        "            data_loader:\n",
        "                a list whose elements are lists of integers.\n",
        "            verbose:\n",
        "                a boolean variable, print loglikelihood of each step if true.\n",
        "        Returns:\n",
        "            loglikelihood_list:\n",
        "                a list of float, stores all the loglikelihood during the fitting\n",
        "                procedure.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Step 1: initialise the parameters for HMM model\n",
        "        initial, transition, emission = self._initial_params()\n",
        "        self.model = HMM(np.log(initial), \n",
        "                         np.log(transition), \n",
        "                         np.log(emission)\n",
        "                         )\n",
        "        \n",
        "        # Step 2: set up the following variables for repeating the e/m-steps.\n",
        "        stop = False                    # flag for stopping the loop\n",
        "        step = 0                        # track the number of steps\n",
        "        delta_param = math.inf          # track the change of parameters\n",
        "        delta_loglikelihood = math.inf  # track the change of log-likelihood\n",
        "        last_loglikelihood = 0.\n",
        "        loglikelihood_list = []\n",
        "        \n",
        "        # Step 3: repeat the e/m-steps\n",
        "        while not stop:\n",
        "            # step 3.1: e-step\n",
        "            loglikelihood, hk_list, hkk_list = self._e_step(data_loader)\n",
        "            # step 3.2: m-step\n",
        "            _initial_, _transition_, _emission_ = \\\n",
        "                self._m_step(data_loader, hk_list, hkk_list)\n",
        "            \n",
        "            # step 3.3: track step and change of parameters/log-likelihoods\n",
        "            step += 1\n",
        "            delta_param = self.model.get_delta_param(\n",
        "                                        np.log(_initial_),\n",
        "                                        np.log(_transition_),\n",
        "                                        np.log(_emission_)\n",
        "                                    )\n",
        "            delta_loglikelihood = abs(loglikelihood - last_loglikelihood)\n",
        "            last_loglikelihood = loglikelihood\n",
        "            \n",
        "            # step 3.4: update the parameters of HMM model\n",
        "            self.model.initial = np.log(_initial_)\n",
        "            self.model.transition = np.log(_transition_)\n",
        "            self.model.emission = np.log(_emission_)\n",
        "            \n",
        "            #step 3.5: check if we're going to end the loop now\n",
        "            stop = self._stop_criterion(step, delta_param, delta_loglikelihood)\n",
        "            \n",
        "            # monitor the learning procedure\n",
        "            loglikelihood_list.append(loglikelihood)\n",
        "            if verbose:\n",
        "                print('step:', step, '\\tloglikelihood:', loglikelihood)\n",
        "\n",
        "        self._trained_ = True\n",
        "        return loglikelihood_list\n",
        "    \n",
        "    def counts(self, data_loader:List) -> None:\n",
        "        \"\"\"Supervised learning method of HMM model, i.e. counting.\n",
        "        \n",
        "        This method would fit the parameters of an HMM model in a supervised \n",
        "        fashion where the optimisation problem is reduced to counting. \n",
        "        \n",
        "        Args:\n",
        "            data_loader:\n",
        "                a list whose element should be a tuple containing two lists of integers.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        _initial_ = np.zeros(self.num_hiddens)\n",
        "        _transition_ = np.zeros([self.num_hiddens, self.num_hiddens]) \n",
        "        _emission_ = np.zeros([self.num_observations, self.num_hiddens])\n",
        "        \n",
        "        for pair in data_loader:\n",
        "            observations = pair[0]\n",
        "            hiddens = pair[1]\n",
        "            \n",
        "            _initial_[hiddens[0]] += 1\n",
        "            _emission_[observations[0]][hiddens[0]] += 1\n",
        "            \n",
        "            for i in range(1, len(hiddens)):\n",
        "                _transition_[hiddens[i]][hiddens[i-1]] += 1\n",
        "                _emission_[observations[i]][hiddens[i]] += 1\n",
        "                \n",
        "        _initial_ / len(data_loader)\n",
        "        _transition_ = _transition_ / _transition_.sum(axis=0, keepdims=True)\n",
        "        _emission_ = _emission_ / _emission_.sum(axis=0, keepdims=True)\n",
        "        \n",
        "        self.model = HMM(np.log(_initial_), \n",
        "                         np.log(_transition_), \n",
        "                         np.log(_emission_)\n",
        "                         )\n",
        "    \n",
        "    def get_trained_model(self) -> HMM:\n",
        "        \"\"\"Return the HMM model with fitted parameters\n",
        "        \n",
        "        This method will first check if the model has been trained. If so, it \n",
        "        will return the model as an object of class HMM from hmm.py.\n",
        "        \n",
        "        Args:\n",
        "            None\n",
        "            \n",
        "        Returns:\n",
        "            self.model:\n",
        "                an instance of HMM class from hmm.py\n",
        "        \"\"\"\n",
        "        \n",
        "        assert self.model is not None and self._trained_, \\\n",
        "            \"The model has not been trained yet!\"\n",
        "        return self.model"
      ],
      "metadata": {
        "id": "YdaXBNYn5q7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1234)\n"
      ],
      "metadata": {
        "id": "jOxhRLPY9fXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: specify the parameters for synthesising data\n",
        "# there are 2 hidden states, and 3 possible observed states where observation\n",
        "# `0` means the end of sequence (<EOS>).\n",
        "initial = np.array([0.2, 0.8])\n",
        "transition = np.array([[0.2, 0.8],\n",
        "                       [0.6, 0.4]])\n",
        "emission = np.array([[0.0, 0.1], # probability of emitting <EOS>\n",
        "                     [0.3, 0.8],\n",
        "                     [0.7, 0.1]])\n",
        "\n",
        "# Step 2: synthesise sequences\n",
        "dataloader = DataLoader(initial=initial,\n",
        "                        transition=transition,\n",
        "                        emission=emission)\n",
        "\n",
        "data_list = dataloader.get_data_list(300)\n",
        "\n",
        "# Step 3: fit an HMM by using HMMOptimiser class\n",
        "optim = HMMOptimiser(num_hiddens=2, num_observations=3)\n",
        "_ = optim.baum_welch(data_list)\n",
        "hmm = optim.model\n",
        "\n",
        "# Step 4: print the parameters fit on the synthetic data\n",
        "# Note that parameters of our HMM class are in the log space\n",
        "print('true initial:\\n', initial)\n",
        "print('fitted initial:\\n', np.exp(hmm.initial))\n",
        "print('true transition:\\n', transition)\n",
        "print('fitted transition:\\n', np.exp(hmm.transition))\n",
        "print('true emission:\\n', emission)\n",
        "print('fitted emission:\\n', np.exp(hmm.emission))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rymFN5M934f",
        "outputId": "b18aa9a2-66f0-43d6-cc1a-2cbfe6a3e6e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 1 \tloglikelihood: -14.981079461366814\n",
            "step: 2 \tloglikelihood: -14.053867634844227\n",
            "step: 3 \tloglikelihood: -14.053738674535843\n",
            "step: 4 \tloglikelihood: -14.053584575719285\n",
            "step: 5 \tloglikelihood: -14.053394346307233\n",
            "step: 6 \tloglikelihood: -14.053153174451314\n",
            "step: 7 \tloglikelihood: -14.052841094665018\n",
            "step: 8 \tloglikelihood: -14.052431227574118\n",
            "step: 9 \tloglikelihood: -14.051887486901876\n",
            "step: 10 \tloglikelihood: -14.051161646131646\n",
            "step: 11 \tloglikelihood: -14.050189682690764\n",
            "step: 12 \tloglikelihood: -14.048887396145068\n",
            "step: 13 \tloglikelihood: -14.047145472549225\n",
            "step: 14 \tloglikelihood: -14.04482450498228\n",
            "step: 15 \tloglikelihood: -14.041751063359504\n",
            "step: 16 \tloglikelihood: -14.037716807428477\n",
            "step: 17 \tloglikelihood: -14.032483834924248\n",
            "step: 18 \tloglikelihood: -14.025800662989331\n",
            "step: 19 \tloglikelihood: -14.01743362695463\n",
            "step: 20 \tloglikelihood: -14.007216474586672\n",
            "step: 21 \tloglikelihood: -13.995114591870875\n",
            "step: 22 \tloglikelihood: -13.98128904952611\n",
            "step: 23 \tloglikelihood: -13.966133474803149\n",
            "step: 24 \tloglikelihood: -13.950253799131012\n",
            "step: 25 \tloglikelihood: -13.93437793205927\n",
            "step: 26 \tloglikelihood: -13.919217026248388\n",
            "step: 27 \tloglikelihood: -13.905329835612031\n",
            "step: 28 \tloglikelihood: -13.893041264175771\n",
            "step: 29 \tloglikelihood: -13.882434952321109\n",
            "step: 30 \tloglikelihood: -13.873403728381705\n",
            "step: 31 \tloglikelihood: -13.865725174236164\n",
            "step: 32 \tloglikelihood: -13.85913418821564\n",
            "step: 33 \tloglikelihood: -13.85337757948904\n",
            "step: 34 \tloglikelihood: -13.848246872229478\n",
            "step: 35 \tloglikelihood: -13.843591732106809\n",
            "step: 36 \tloglikelihood: -13.839318941958815\n",
            "step: 37 \tloglikelihood: -13.83538230364484\n",
            "step: 38 \tloglikelihood: -13.831768280636304\n",
            "step: 39 \tloglikelihood: -13.828481184162863\n",
            "step: 40 \tloglikelihood: -13.825530532794149\n",
            "step: 41 \tloglikelihood: -13.822922033649617\n",
            "step: 42 \tloglikelihood: -13.82065256159263\n",
            "step: 43 \tloglikelihood: -13.818708674892035\n",
            "step: 44 \tloglikelihood: -13.817067700508472\n",
            "step: 45 \tloglikelihood: -13.815700270968184\n",
            "step: 46 \tloglikelihood: -13.814573327535209\n",
            "step: 47 \tloglikelihood: -13.813652894295785\n",
            "step: 48 \tloglikelihood: -13.81290624748488\n",
            "step: 49 \tloglikelihood: -13.812303367146233\n",
            "step: 50 \tloglikelihood: -13.811817728874894\n",
            "step: 51 \tloglikelihood: -13.811426576326234\n",
            "step: 52 \tloglikelihood: -13.811110834341756\n",
            "step: 53 \tloglikelihood: -13.810854805091362\n",
            "step: 54 \tloglikelihood: -13.810645757111349\n",
            "step: 55 \tloglikelihood: -13.810473483074796\n",
            "step: 56 \tloglikelihood: -13.810329873255617\n",
            "step: 57 \tloglikelihood: -13.810208530058274\n",
            "step: 58 \tloglikelihood: -13.8101044343282\n",
            "step: 59 \tloglikelihood: -13.810013665065199\n",
            "step: 60 \tloglikelihood: -13.809933169086404\n",
            "step: 61 \tloglikelihood: -13.809860574778392\n",
            "step: 62 \tloglikelihood: -13.809794043316348\n",
            "step: 63 \tloglikelihood: -13.8097321508954\n",
            "step: 64 \tloglikelihood: -13.809673796150962\n",
            "step: 65 \tloglikelihood: -13.809618127757368\n",
            "step: 66 \tloglikelihood: -13.809564488025778\n",
            "step: 67 \tloglikelihood: -13.809512369091557\n",
            "step: 68 \tloglikelihood: -13.809461378951655\n",
            "step: 69 \tloglikelihood: -13.809411215176432\n",
            "step: 70 \tloglikelihood: -13.809361644582019\n",
            "step: 71 \tloglikelihood: -13.80931248752225\n",
            "step: 72 \tloglikelihood: -13.80926360575453\n",
            "step: 73 \tloglikelihood: -13.809214893068232\n",
            "step: 74 \tloglikelihood: -13.809166268046704\n",
            "step: 75 \tloglikelihood: -13.809117668476386\n",
            "step: 76 \tloglikelihood: -13.80906904702737\n",
            "step: 77 \tloglikelihood: -13.809020367915315\n",
            "step: 78 \tloglikelihood: -13.808971604320883\n",
            "step: 79 \tloglikelihood: -13.808922736394003\n",
            "step: 80 \tloglikelihood: -13.808873749709791\n",
            "step: 81 \tloglikelihood: -13.808824634073298\n",
            "step: 82 \tloglikelihood: -13.808775382593865\n",
            "step: 83 \tloglikelihood: -13.808725990967812\n",
            "step: 84 \tloglikelihood: -13.808676456922127\n",
            "step: 85 \tloglikelihood: -13.808626779782875\n",
            "step: 86 \tloglikelihood: -13.80857696013986\n",
            "step: 87 \tloglikelihood: -13.808526999585965\n",
            "step: 88 \tloglikelihood: -13.808476900514167\n",
            "step: 89 \tloglikelihood: -13.808426665959244\n",
            "step: 90 \tloglikelihood: -13.808376299474178\n",
            "step: 91 \tloglikelihood: -13.808325805033245\n",
            "step: 92 \tloglikelihood: -13.808275186955992\n",
            "step: 93 \tloglikelihood: -13.808224449847236\n",
            "step: 94 \tloglikelihood: -13.808173598549553\n",
            "step: 95 \tloglikelihood: -13.808122638105418\n",
            "step: 96 \tloglikelihood: -13.808071573726853\n",
            "step: 97 \tloglikelihood: -13.808020410770844\n",
            "step: 98 \tloglikelihood: -13.807969154719284\n",
            "step: 99 \tloglikelihood: -13.807917811162389\n",
            "step: 100 \tloglikelihood: -13.807866385784783\n",
            "true initial:\n",
            " [0.2 0.8]\n",
            "fitted initial:\n",
            " [0.40847249 0.59152751]\n",
            "true transition:\n",
            " [[0.2 0.8]\n",
            " [0.6 0.4]]\n",
            "fitted transition:\n",
            " [[0.43027886 0.56972114]\n",
            " [0.94985613 0.05014387]]\n",
            "true emission:\n",
            " [[0.  0.1]\n",
            " [0.3 0.8]\n",
            " [0.7 0.1]]\n",
            "fitted emission:\n",
            " [[0.04383897 0.08653888]\n",
            " [0.4066316  0.87874284]\n",
            " [0.54952943 0.03471828]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the HMM model is non-identifiable in general. Specifically, it means that you could permute the hidden state values $h$ and the corresponding matrices $\\mathbf{a}, \\mathbf{A}$, and $\\mathbf{B}$ and obtain the same log-likelihood. As we see the hidden states in the learnt model above were permuted, i.e. the order of the values in the fitted parameters $\\hat{\\mathbf{a}}$ corresponds to a reversed order compared to the true parameter $\\mathbf{a}$, and an equivalent permutation can also be seen in the fitted $\\hat{\\mathbf{A}}$ and $\\hat{\\mathbf{B}}$ matrices compared to the true ones $\\mathbf{A}$ and $\\mathbf{B}$."
      ],
      "metadata": {
        "id": "IYzZzw1Q-V05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tips on the sanity check of your implementation\n",
        "\n",
        "Since the Baum-Welch algorithm can only guarantee that the model converges to a **local optima**, the learnt parameters might not be exactly the same to the true parameters used to synthesise data.\n",
        "But, if the implementation is correct, we should be able to observe all of the following phenomena during learning:\n",
        " 1. *Non-decreasing likelihood of data*: the (log-)likelihood of the sequences should never decrease during the iteration of E- and M-steps;\n",
        " 2. *Variance of the learnt parameter distance to the true parameters decreases with more data*: the more observed sequences we have for learning, the smaller the variance of the distance between the learnt parameter values and the true values.\n",
        " 3. *Parameters do not diverge if true parameters are used in initialisation*: if we use the true parameters to initialise the parameters of the `HMM`, the parameters should stay around the initial values during learning. Note that the true parameters might not be the optimal values for a particular dataset sample due to the finite sample size.\n",
        "\n",
        "In the following cell we provide a function to verify the first two cases in the above list."
      ],
      "metadata": {
        "id": "BKoMJFfy-c0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_true_params() -> Tuple[ArrayLike]:\n",
        "    initial = np.array([0.2, 0.8])\n",
        "    transition = np.array([[0.2, 0.8],\n",
        "                           [0.6, 0.4]])\n",
        "    emission = np.array([[0.0, 0.1], # probability of emitting <EOS>\n",
        "                         [0.3, 0.8],\n",
        "                         [0.7, 0.1]])\n",
        "    return initial, transition, emission\n",
        "\n",
        "def get_dist_to_true_params(params:Tuple[ArrayLike], \n",
        "                            true_params:Tuple[ArrayLike]) -> float: \n",
        "    # Compute the Euclidean distance (L2 norm) between the learnt parameters and the true parameters\n",
        "    \n",
        "    # To handle the non-identifiability of the HMM, for the 2D hiddens case\n",
        "    # we simply consider the two possible orderings and keep whichever is smaller\n",
        "    true_params_stacked = np.vstack([true_params[0],\n",
        "                                     true_params[1],\n",
        "                                     true_params[2]]).flatten()\n",
        "    true_params_permuted = np.vstack([np.flip(true_params[0]),\n",
        "                                      np.flip(true_params[1]),\n",
        "                                      true_params[2][:, ::-1]]).flatten()\n",
        "    \n",
        "    params_vec = np.vstack([params[0], params[1], params[2]]).flatten()\n",
        "    \n",
        "    return min(np.linalg.norm(params_vec - true_params_stacked, ord=2),\n",
        "               np.linalg.norm(params_vec - true_params_permuted, ord=2))\n",
        "\n",
        "def sanity_check() -> List:\n",
        "    num_sample_list = [50, 500]\n",
        "    num_runs = 50\n",
        "\n",
        "    true_params = get_true_params()\n",
        "    dataloader = DataLoader(initial=true_params[0],\n",
        "                            transition=true_params[1],\n",
        "                            emission=true_params[2])\n",
        "    data_list = dataloader.get_data_list(max(num_sample_list))\n",
        "\n",
        "    result_list = []\n",
        "    for num_samples in num_sample_list:\n",
        "        # Only use the first num_samples data-points from the data_list\n",
        "        dl = data_list[:num_samples]\n",
        "        for i in range(num_runs):\n",
        "            print(f'Starting run {i}/{num_runs} with data size {num_samples}.')\n",
        "            optim = HMMOptimiser(num_hiddens=2,\n",
        "                                 num_observations=3)\n",
        "            loglikelihood_list = optim.baum_welch(dl, verbose=False)\n",
        "            nondec_loglike = all(x<=y for x, y in zip(loglikelihood_list[:-1], loglikelihood_list[1:]))\n",
        "            params = (np.exp(optim.model.initial),\n",
        "                      np.exp(optim.model.transition),\n",
        "                      np.exp(optim.model.emission))\n",
        "            dist = get_dist_to_true_params(params, true_params)\n",
        "\n",
        "            result = {\n",
        "                'num_samples': num_samples,\n",
        "                'distance': dist,\n",
        "                'non_decreasing': 1 if nondec_loglike else -1\n",
        "            }\n",
        "            result_list.append(result)\n",
        "\n",
        "    return result_list"
      ],
      "metadata": {
        "id": "JCmsJ7XI-hsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can run the sanity check. Note that it may take quite a while to run the following sanity check as it repeats the EM fitting algorithm 50 times for sample sizes of 50 and 500, hence running a total of 100 runs."
      ],
      "metadata": {
        "id": "SRi5vhXp-oCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_list = sanity_check()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RPAvjyh1-ptT",
        "outputId": "9e8e0ea5-b26d-485d-81e2-7afde909a77b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting run 0/50 with data size 50.\n",
            "Starting run 1/50 with data size 50.\n",
            "Starting run 2/50 with data size 50.\n",
            "Starting run 3/50 with data size 50.\n",
            "Starting run 4/50 with data size 50.\n",
            "Starting run 5/50 with data size 50.\n",
            "Starting run 6/50 with data size 50.\n",
            "Starting run 7/50 with data size 50.\n",
            "Starting run 8/50 with data size 50.\n",
            "Starting run 9/50 with data size 50.\n",
            "Starting run 10/50 with data size 50.\n",
            "Starting run 11/50 with data size 50.\n",
            "Starting run 12/50 with data size 50.\n",
            "Starting run 13/50 with data size 50.\n",
            "Starting run 14/50 with data size 50.\n",
            "Starting run 15/50 with data size 50.\n",
            "Starting run 16/50 with data size 50.\n",
            "Starting run 17/50 with data size 50.\n",
            "Starting run 18/50 with data size 50.\n",
            "Starting run 19/50 with data size 50.\n",
            "Starting run 20/50 with data size 50.\n",
            "Starting run 21/50 with data size 50.\n",
            "Starting run 22/50 with data size 50.\n",
            "Starting run 23/50 with data size 50.\n",
            "Starting run 24/50 with data size 50.\n",
            "Starting run 25/50 with data size 50.\n",
            "Starting run 26/50 with data size 50.\n",
            "Starting run 27/50 with data size 50.\n",
            "Starting run 28/50 with data size 50.\n",
            "Starting run 29/50 with data size 50.\n",
            "Starting run 30/50 with data size 50.\n",
            "Starting run 31/50 with data size 50.\n",
            "Starting run 32/50 with data size 50.\n",
            "Starting run 33/50 with data size 50.\n",
            "Starting run 34/50 with data size 50.\n",
            "Starting run 35/50 with data size 50.\n",
            "Starting run 36/50 with data size 50.\n",
            "Starting run 37/50 with data size 50.\n",
            "Starting run 38/50 with data size 50.\n",
            "Starting run 39/50 with data size 50.\n",
            "Starting run 40/50 with data size 50.\n",
            "Starting run 41/50 with data size 50.\n",
            "Starting run 42/50 with data size 50.\n",
            "Starting run 43/50 with data size 50.\n",
            "Starting run 44/50 with data size 50.\n",
            "Starting run 45/50 with data size 50.\n",
            "Starting run 46/50 with data size 50.\n",
            "Starting run 47/50 with data size 50.\n",
            "Starting run 48/50 with data size 50.\n",
            "Starting run 49/50 with data size 50.\n",
            "Starting run 0/50 with data size 500.\n",
            "Starting run 1/50 with data size 500.\n",
            "Starting run 2/50 with data size 500.\n",
            "Starting run 3/50 with data size 500.\n",
            "Starting run 4/50 with data size 500.\n",
            "Starting run 5/50 with data size 500.\n",
            "Starting run 6/50 with data size 500.\n",
            "Starting run 7/50 with data size 500.\n",
            "Starting run 8/50 with data size 500.\n",
            "Starting run 9/50 with data size 500.\n",
            "Starting run 10/50 with data size 500.\n",
            "Starting run 11/50 with data size 500.\n",
            "Starting run 12/50 with data size 500.\n",
            "Starting run 13/50 with data size 500.\n",
            "Starting run 14/50 with data size 500.\n",
            "Starting run 15/50 with data size 500.\n",
            "Starting run 16/50 with data size 500.\n",
            "Starting run 17/50 with data size 500.\n",
            "Starting run 18/50 with data size 500.\n",
            "Starting run 19/50 with data size 500.\n",
            "Starting run 20/50 with data size 500.\n",
            "Starting run 21/50 with data size 500.\n",
            "Starting run 22/50 with data size 500.\n",
            "Starting run 23/50 with data size 500.\n",
            "Starting run 24/50 with data size 500.\n",
            "Starting run 25/50 with data size 500.\n",
            "Starting run 26/50 with data size 500.\n",
            "Starting run 27/50 with data size 500.\n",
            "Starting run 28/50 with data size 500.\n",
            "Starting run 29/50 with data size 500.\n",
            "Starting run 30/50 with data size 500.\n",
            "Starting run 31/50 with data size 500.\n",
            "Starting run 32/50 with data size 500.\n",
            "Starting run 33/50 with data size 500.\n",
            "Starting run 34/50 with data size 500.\n",
            "Starting run 35/50 with data size 500.\n",
            "Starting run 36/50 with data size 500.\n",
            "Starting run 37/50 with data size 500.\n",
            "Starting run 38/50 with data size 500.\n",
            "Starting run 39/50 with data size 500.\n",
            "Starting run 40/50 with data size 500.\n",
            "Starting run 41/50 with data size 500.\n",
            "Starting run 42/50 with data size 500.\n",
            "Starting run 43/50 with data size 500.\n",
            "Starting run 44/50 with data size 500.\n",
            "Starting run 45/50 with data size 500.\n",
            "Starting run 46/50 with data size 500.\n",
            "Starting run 47/50 with data size 500.\n",
            "Starting run 48/50 with data size 500.\n",
            "Starting run 49/50 with data size 500.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-f6c4af17abe2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-064ca3e0e14b>\u001b[0m in \u001b[0;36msanity_check\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m             optim = HMMOptimiser(num_hiddens=2,\n\u001b[1;32m     45\u001b[0m                                  num_observations=3)\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mloglikelihood_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbaum_welch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mnondec_loglike\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0my\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloglikelihood_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloglikelihood_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             params = (np.exp(optim.model.initial),\n",
            "\u001b[0;32m<ipython-input-4-b5212154a6f4>\u001b[0m in \u001b[0;36mbaum_welch\u001b[0;34m(self, data_loader, verbose)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;31m# step 3.1: e-step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m             \u001b[0mloglikelihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhk_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhkk_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_e_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m             \u001b[0;31m# step 3.2: m-step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0m_initial_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_transition_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_emission_\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-b5212154a6f4>\u001b[0m in \u001b[0;36m_e_step\u001b[0;34m(self, data_loader)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mo_seq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhkk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarginal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m             \u001b[0mhk_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mhkk_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhkk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-523a3dd904fa>\u001b[0m in \u001b[0;36mmarginal\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \"\"\"\n\u001b[1;32m    149\u001b[0m         \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_px\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0mnode_marginal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlog_px\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-523a3dd904fa>\u001b[0m in \u001b[0;36m_backward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0memission_t_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memission\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             beta[t] = logsumexp(beta[t + 1].reshape(1, N) + \n\u001b[0m\u001b[1;32m     86\u001b[0m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransition\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                                 \u001b[0memission_t_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/scipy/special/_logsumexp.py\u001b[0m in \u001b[0;36mlogsumexp\u001b[0;34m(a, axis, b, keepdims, return_sign)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0ma_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ma_max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2789\u001b[0m     \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2790\u001b[0m     \"\"\"\n\u001b[0;32m-> 2791\u001b[0;31m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[0m\u001b[1;32m   2792\u001b[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[1;32m   2793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "distances = {r['num_samples']: [] for r in result_list}\n",
        "for r in result_list:\n",
        "    distances[r['num_samples']].append(r['distance'])\n",
        "    \n",
        "fig, ax = plt.subplots()\n",
        "    \n",
        "for i, (num_samples, dist) in enumerate(distances.items()):\n",
        "    mean_dist = np.mean(dist)\n",
        "    std_err = np.std(dist, ddof=1)/(len(dist)**(0.5))\n",
        "    ax.errorbar(num_samples, mean_dist, yerr=std_err, marker='_', capsize=50, markersize=30)\n",
        "    print(f'Num samples: {num_samples}, mean {mean_dist}, std. err. {std_err}')\n",
        "    \n",
        "ax.set_xlim(-200, 750)\n",
        "ax.set_xticks(ticks=list(distances.keys()), labels=list(distances.keys()))\n",
        "ax.set_ylabel('distance')\n",
        "ax.set_xlabel('# samples')"
      ],
      "metadata": {
        "id": "mfrQjRb4PpKJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}